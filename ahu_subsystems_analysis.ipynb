{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy-NQgsXiQO3"
      },
      "source": [
        "# Notes:\n",
        "*   <mark>For confidentiality reasons</mark>, no figure is shown as a result of any block execution and the actual dataset naming and input/output addresses are replaced by aliases.\n",
        "*   The lookup tables implemented in the notebook are all taken from \"<mark>Van der Horst, S. A. M. (2019). Economically optimizing maintenance of air handling units (Masterâ€™s thesis). Technische Universiteit Eindhoven</mark>\".\n",
        "*   The ARM approach implemented in this notebook was inspired and mainly learned from: <mark>Isaiah Hull. Market Basket Analysis in Python. https://www.datacamp.com/courses/\n",
        "market-basket-analysis-in-python</mark>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7595UIXVFovr"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPUgLKbTr4tE"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzcugnMcFtfO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utbh3w3Nr8jG"
      },
      "source": [
        "## Download Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAqZ8u70Fxwc"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jed0F0KsC0F"
      },
      "source": [
        "## Implement Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8q14WZMF0rB"
      },
      "outputs": [],
      "source": [
        "def correct_types(df, columns):\n",
        "    \"\"\"\n",
        "    Preprocess the specified columns in a DataFrame by replacing NaN values with an empty string\n",
        "    and converting non-string values to string objects.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The input DataFrame.\n",
        "    - column (str): The column to be preprocessed. Default is 'Description'.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The DataFrame with the specified column preprocessed.\n",
        "    \"\"\"\n",
        "\n",
        "    df[columns] = df[columns].fillna('')\n",
        "    df[columns] = df[columns].astype(str)\n",
        "    return df\n",
        "\n",
        "def replace_punctuation_text(text):\n",
        "    \"\"\"\n",
        "    Replace punctuation in the input text.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The input text in which punctuation will be replaced.\n",
        "\n",
        "    Returns:\n",
        "    - str: The input text with punctuation replaced.\n",
        "    \"\"\"\n",
        "    # Replace \"'s\" with empty space.\n",
        "    text = text.replace(\"'s\", '')\n",
        "\n",
        "    # Create a translation table to replace '.' and \"'\" with empty space, and other punctuation with spaces.\n",
        "    translator = str.maketrans({'.': '', \"'\": '', **{p: ' ' for p in string.punctuation if p not in ['.', \"'\"]}})\n",
        "\n",
        "    cleaned_text = text.translate(translator)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "def replace_punctuation(df, columns):\n",
        "    \"\"\"\n",
        "    Replaces specific punctuation in the specified columns of a DataFrame.\n",
        "\n",
        "    This function is designed to clean textual data in the specified columns of a DataFrame by:\n",
        "    - Replacing occurrences of \"'s\" with an empty string.\n",
        "    - Replacing periods (.) and apostrophes (') with an empty string.\n",
        "    - Replacing other punctuation marks with a space.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing the columns to be cleaned.\n",
        "        columns (list or str): The column name(s) of the DataFrame where the replacements should be applied.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with the specified columns cleaned of certain punctuation marks.\n",
        "    \"\"\"\n",
        "    # Replace \"'s\" with an empty string in the specified columns.\n",
        "    df[columns] = df[columns].replace(\"'s\", '', regex=True)\n",
        "\n",
        "    # Create a translation table to replace '.' and \"'\" with empty space,\n",
        "    # and other punctuation with spaces.\n",
        "    translator = str.maketrans({\n",
        "        '.': '',                # Remove periods\n",
        "        \"'\": '',                # Remove apostrophes\n",
        "        **{p: ' ' for p in string.punctuation if p not in ['.', \"'\"]}  # Replace other punctuation with space\n",
        "    })\n",
        "\n",
        "    # Apply the translation table to each element in the specified columns.\n",
        "    df[columns] = df[columns].applymap(lambda x: x.translate(translator))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def remove_numerical_values_text(text):\n",
        "    \"\"\"\n",
        "    Remove numbers and model-related patterns from the input text.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The input text from which numbers and model-related patterns will be removed.\n",
        "\n",
        "    Returns:\n",
        "    - str: The input text with numbers and model-related patterns removed.\n",
        "    \"\"\"\n",
        "    # Remove standalone numbers with optional floating points; such as 154, 99.31.\n",
        "    text = re.sub(r'\\b\\d+(\\.\\d+)?\\b', '', text)\n",
        "\n",
        "    # Remove ordinal numbers; such as 1e, 2ste, 3de, 4e.\n",
        "    text = re.sub(r'\\b\\d+(e|ste|de|e)\\b', '', text)\n",
        "\n",
        "    # Remove numerical quantifiers; such as 1x OH.\n",
        "    text = re.sub(r'\\b\\d+[xX]\\b', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_numerical_values(df, columns):\n",
        "    \"\"\"\n",
        "    Removes various forms of numerical values from the specified columns in a DataFrame.\n",
        "\n",
        "    This function cleans textual data in the specified columns by:\n",
        "    - Removing standalone numbers with optional floating points (e.g., 154, 99.31).\n",
        "    - Removing ordinal numbers commonly used in Dutch (e.g., 1e, 2ste, 3de, 4e).\n",
        "    - Removing numerical quantifiers with 'x' (e.g., 1x, 2X).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing the columns to be cleaned.\n",
        "        columns (list or str): The column name(s) of the DataFrame where the numerical values should be removed.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with the specified columns cleaned of numerical values.\n",
        "    \"\"\"\n",
        "    # Remove standalone numbers with optional floating points (e.g., 154, 99.31).\n",
        "    df[columns] = df[columns].applymap(lambda x: re.sub(r'\\b\\d+(\\.\\d+)?\\b', '', x))\n",
        "\n",
        "    # Remove ordinal numbers (e.g., 1e, 2ste, 3de, 4e).\n",
        "    df[columns] = df[columns].applymap(lambda x: re.sub(r'\\b\\d+(e|ste|de)\\b', '', x))\n",
        "\n",
        "    # Remove numerical quantifiers (e.g., 1x, 2X).\n",
        "    df[columns] = df[columns].applymap(lambda x: re.sub(r'\\b\\d+[xX]\\b', '', x))\n",
        "\n",
        "    return df\n",
        "\n",
        "def stemize(text):\n",
        "    \"\"\"\n",
        "    Perform stemming on the input text using the Dutch Snowball Stemmer.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The input text to be stemmed.\n",
        "\n",
        "    Returns:\n",
        "    str: The stemmed text.\n",
        "\n",
        "    Example:\n",
        "    >>> stemize(\"This is an example text for stemming.\")\n",
        "    'thi is an exampl text for stem.'\n",
        "    \"\"\"\n",
        "    stemmer = SnowballStemmer(\"dutch\")\n",
        "    tokens = word_tokenize(text)\n",
        "    text = ' '.join([stemmer.stem(token) for token in tokens])\n",
        "    return text\n",
        "\n",
        "def normalize_lookups():\n",
        "    \"\"\"\n",
        "    Normalize and preprocess the global lookup lists.\n",
        "\n",
        "    This function performs the following operations on each global lookup list:\n",
        "    1. Orders lookup lists by length in descending order.\n",
        "    2. Initializes the Dutch Snowball Stemmer.\n",
        "    3. Replaces punctuations using the `replace_punctuation` function.\n",
        "    4. Removes numerical values using the `remove_numerical_values` function.\n",
        "    5. Applies stemming using the `stemize` function.\n",
        "    6. Converts the resulting list to a set.\n",
        "\n",
        "    Note: The original lookup lists are modified in-place.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Get all lookup lists using regular expression.\n",
        "    lookup_lists = [var for var in globals() if re.match(r'^lookup_', var)]\n",
        "    lookup_ventilation_lists = [var for var in globals() if re.match(r'^sublookup_', var)]\n",
        "\n",
        "    for lookup_list_name in lookup_lists:\n",
        "\n",
        "        lookup_list = globals()[lookup_list_name]\n",
        "\n",
        "        # Replace punctuations, remove numerical values, and apply stemming.\n",
        "        normalized_list = [stemize(remove_numerical_values_text(replace_punctuation_text(word))) for word in lookup_list]\n",
        "\n",
        "        # Convert to set.\n",
        "        lookup_list.clear()\n",
        "        lookup_list.extend(set(normalized_list))\n",
        "\n",
        "    for lookup_list_name in lookup_ventilation_lists:\n",
        "\n",
        "        lookup_list = globals()[lookup_list_name]\n",
        "\n",
        "        # Replace punctuations, remove numerical values, and apply stemming.\n",
        "        normalized_list = [stemize(remove_numerical_values_text(replace_punctuation_text(word))) for word in lookup_list]\n",
        "\n",
        "        # Convert to set.\n",
        "        lookup_list.clear()\n",
        "        lookup_list.extend(set(normalized_list))\n",
        "\n",
        "def group_modules(description, lookup_lists):\n",
        "    \"\"\"\n",
        "    Classifies a description into a module type based on predefined lookup lists.\n",
        "\n",
        "    This function checks if the given description contains any words from a set of lookup lists\n",
        "    and assigns a module type accordingly. The lookup lists are accessed dynamically using their\n",
        "    names, and the corresponding module type is identified using a separate mapping.\n",
        "\n",
        "    Args:\n",
        "        description (str): The text description to be classified.\n",
        "        lookup_lists (list of str): A list of lookup list names to check against the description.\n",
        "\n",
        "    Returns:\n",
        "        str: The identified module type name if a match is found; otherwise, 'Other'.\n",
        "    \"\"\"\n",
        "    # Convert the description to lowercase to ensure case-insensitive matching.\n",
        "    description = description.lower()\n",
        "\n",
        "    # Iterate through each lookup list name in the provided list.\n",
        "    for lookup_list_name in lookup_lists:\n",
        "        # Access the actual lookup list using the global variable name.\n",
        "        lookup_list = globals().get(lookup_list_name)\n",
        "\n",
        "        # Check if any word in the lookup list is present in the description.\n",
        "        for word in lookup_list:\n",
        "            if word.lower() in description:\n",
        "                # Map the lookup list name to its corresponding module type.\n",
        "                type_name = module_names[lookup_list_name.split('_')[1]]\n",
        "                return type_name  # Return the matched module type.\n",
        "\n",
        "    # Return 'Other' if no matching word is found in the lookup lists.\n",
        "    return 'Other'\n",
        "\n",
        "def group_submodules(description, lookup_lists):\n",
        "    \"\"\"\n",
        "    Classifies a description into a submodule type based on predefined lookup lists.\n",
        "\n",
        "    This function checks if the given description contains any words from a set of lookup lists\n",
        "    and assigns a submodule type accordingly. The lookup lists are accessed dynamically using their\n",
        "    names, and the corresponding submodule type is identified using a separate mapping.\n",
        "\n",
        "    Args:\n",
        "        description (str): The text description to be classified.\n",
        "        lookup_lists (list of str): A list of lookup list names to check against the description.\n",
        "\n",
        "    Returns:\n",
        "        str: The identified submodule type name if a match is found; otherwise, 'Uncategorized'.\n",
        "    \"\"\"\n",
        "    # Convert the description to lowercase to ensure case-insensitive matching.\n",
        "    description = description.lower()\n",
        "\n",
        "    # Iterate through each lookup list name in the provided list.\n",
        "    for lookup_list_name in lookup_lists:\n",
        "        # Access the actual lookup list using the global variable name.\n",
        "        lookup_list = globals().get(lookup_list_name)\n",
        "\n",
        "        # Check if any word in the lookup list is present in the description.\n",
        "        for word in lookup_list:\n",
        "            if word.lower() in description:\n",
        "                # Map the lookup list name to its corresponding submodule type.\n",
        "                type_name = submodule_names[lookup_list_name.split('_')[2]]\n",
        "                return type_name  # Return the matched submodule type.\n",
        "\n",
        "    # Return 'Uncategorized' if no matching word is found in the lookup lists.\n",
        "    return 'Uncategorized'\n",
        "\n",
        "def rules_to_coordinates(rules):\n",
        "    \"\"\"\n",
        "    Convert association rules to coordinates.\n",
        "\n",
        "    Parameters:\n",
        "    - rules (pd.DataFrame): DataFrame containing association rules with 'antecedents', 'consequents', and other columns.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: DataFrame with 'antecedent', 'consequent', and 'rule' columns representing coordinates.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the first item from antecedents and consequents.\n",
        "    rules['antecedent'] = rules['antecedents'].apply(lambda antecedent: list(antecedent)[0])\n",
        "    rules['consequent'] = rules['consequents'].apply(lambda consequent: list(consequent)[0])\n",
        "\n",
        "    # Assign rule index to a new column 'rule'.\n",
        "    rules['rule'] = rules.index\n",
        "\n",
        "    # Select relevant columns for coordinates.\n",
        "    coords = rules[['antecedent', 'consequent', 'rule']]\n",
        "\n",
        "    return coords\n",
        "\n",
        "def contains_word_regex(sentence, target_word):\n",
        "    \"\"\"\n",
        "    Check if a given word is present in a sentence using regex.\n",
        "\n",
        "    Parameters:\n",
        "    - sentence (str): The input sentence to check.\n",
        "    - target_word (str): The word to look for in the sentence.\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if the word is found, False otherwise.\n",
        "    \"\"\"\n",
        "    # Construct a regex pattern to match the whole word, case-insensitive.\n",
        "    pattern = r'\\b' + re.escape(target_word) + r'\\b'\n",
        "\n",
        "    # Use re.search to find the pattern in the sentence.\n",
        "    match = re.search(pattern, sentence, flags=re.IGNORECASE)\n",
        "\n",
        "    # Return True if a match is found, False otherwise.\n",
        "    return bool(match)\n",
        "\n",
        "def convert_str_to_set(itemset_str):\n",
        "    \"\"\"\n",
        "    Converts a comma-separated string into a set of items.\n",
        "\n",
        "    This function takes a string containing items separated by commas,\n",
        "    strips any surrounding whitespace from each item, and returns a set\n",
        "    containing the unique items. This is useful for converting textual\n",
        "    representations of itemsets into Python set objects for further analysis.\n",
        "\n",
        "    Args:\n",
        "        itemset_str (str): A string of items separated by commas.\n",
        "\n",
        "    Returns:\n",
        "        set: A set containing the stripped items from the input string.\n",
        "    \"\"\"\n",
        "    # Split the string by commas and strip whitespace from each item.\n",
        "    items = [item.strip() for item in itemset_str.split(',')]\n",
        "\n",
        "    # Convert the list of items to a set to ensure uniqueness.\n",
        "    return set(items)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in6RcmHhsRMk"
      },
      "source": [
        "## Define Lookup Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sbfEVkPGB0P"
      },
      "outputs": [],
      "source": [
        "lookup_ventilation = [\n",
        "    \"lbk\",\n",
        "    \"luchtbehandeling\",\n",
        "    \"luchtbehandelen\",\n",
        "    \"luchbehandeling\",\n",
        "    \"luchtbehandeling\",\n",
        "    \"ventilatiesysteem\",\n",
        "    \"ventilatie\",\n",
        "    \"luchtklep\",\n",
        "    \"stoombevochtiger\",\n",
        "    \"stoombevochtiging\",\n",
        "    \"bevochtiger\",\n",
        "    \"toevoerventilator\",\n",
        "    \"afvoerventilator\",\n",
        "    \"ventilatormotor\",\n",
        "    \"dakventilatoren\",\n",
        "    \"Dakventilator\",\n",
        "    \"toevoer ventilator\",\n",
        "    \"dak ventilatoren\",\n",
        "    \"Afzuiventilator\",\n",
        "    \"wiel\",\n",
        "    \"afzuigvent\",\n",
        "    \"V-snaren\",\n",
        "    \"V snaren\",\n",
        "    \"filters\",\n",
        "    \"snaarbreuk\",\n",
        "    \"condensafvoer\",\n",
        "    \"condensor\",\n",
        "    \"filter\",\n",
        "    \"Luchtbeh\",\n",
        "    \"LBH\",\n",
        "    \"vorst\",\n",
        "    \"verwarmingsbatterij\",\n",
        "    \"stoomvochtiger\",\n",
        "    \"luchtdebiet\",\n",
        "    \"luchtzakken\",\n",
        "    \"fancoil\",\n",
        "    \"fan coil\",\n",
        "    \"fan-coil\"\n",
        "]\n",
        "\n",
        "sublookup_ventilation_damper = [\n",
        "    \"kleppen\",\n",
        "    \"luchtklepmotor\",\n",
        "    \"luchtklep\",\n",
        "    \"servo\"\n",
        "]\n",
        "sublookup_ventilation_fan = [\n",
        "    \"lager\",\n",
        "    \"motor\",\n",
        "    \"snaar\",\n",
        "    \"snaren\",\n",
        "    \"vsnaren\",\n",
        "    \"snaarbreuk\",\n",
        "    \"lagers\",\n",
        "    \"lages\",\n",
        "    \"ventilator\",\n",
        "    \"poellies\",\n",
        "    \"poelies\",\n",
        "    \"pullies\"\n",
        "]\n",
        "sublookup_ventilation_bms = [\n",
        "    \"Relais\",\n",
        "    \"Opnemer\",\n",
        "    \"sensor\",\n",
        "    \"kloktijd\",\n",
        "    \"regeling\",\n",
        "    \"inbedrijf\",\n",
        "    \"optoeren\",\n",
        "    \"frequentieregelaar\",\n",
        "    \"inregelen\",\n",
        "    \"tijdklok\",\n",
        "    \"kloktijd\"\n",
        "]\n",
        "sublookup_ventilation_heatExchanger = [\n",
        "    \"warmte wiel\",\n",
        "    \"warmtewiel\",\n",
        "    \"warmtewisselaar\",\n",
        "    \"twinc\"\n",
        "]\n",
        "sublookup_ventilation_airFilter = [\n",
        "    \"filtervuil\",\n",
        "    \"vuilfilter\",\n",
        "    \"filter wisseling\",\n",
        "    \"zakkenfilter\",\n",
        "    \"luchtfilter\",\n",
        "    \"vuil filter\",\n",
        "    \"filters verwijderen\",\n",
        "    \"fiters vervangen\",\n",
        "    \"filters reinigen\",\n",
        "    \"filter OH\",\n",
        "    \"filters\",\n",
        "    \"groffilter\",\n",
        "    \"vervangen filter\",\n",
        "    \"filterwisseling\",\n",
        "    \"filter vervangen\",\n",
        "    \"filterwiseling\",\n",
        "    \"Filtervervanging\"\n",
        "]\n",
        "sublookup_ventilation_humidifier = [\n",
        "    \"bevochtiging\",\n",
        "    \"stoombevochtiging\",\n",
        "    \"stoombevochtiger\",\n",
        "    \"stoombev.\",\n",
        "    \"stoombevochtiger\",\n",
        "    \"stoomvormer\",\n",
        "    \"bevochtiger\"\n",
        "]\n",
        "sublookup_ventilation_casing = [\n",
        "    \"desinfecteren\",\n",
        "    \"reinigen\",\n",
        "    \"coaten\",\n",
        "    \"vervuild\",\n",
        "    \"desinfectie\",\n",
        "    \"Luchtlekkage\"\n",
        "]\n",
        "sublookup_ventilation_regularMaintenance = [\n",
        "    \"OH\",\n",
        "    \"inspectie\",\n",
        "    \"onderhoud\",\n",
        "    \"Preventieve\",\n",
        "    \"Inspecteren\"\n",
        "]\n",
        "sublookup_ventilation_undefinedMalfunctions = [\n",
        "    \"defect\",\n",
        "    \"storing\",\n",
        "    \"luchtbehandeling\",\n",
        "    \"werkt niet\",\n",
        "    \"lbk\",\n",
        "    \"ventilatiesysteem\",\n",
        "    \"ventilatie\",\n",
        "    \"condensafvoer\",\n",
        "    \"LBH\",\n",
        "    \"fan coil\",\n",
        "    \"fan-coil\"\n",
        "]\n",
        "sublookup_ventilation_cooling = [\n",
        "    \"koelblok\",\n",
        "    \"condensor\",\n",
        "    \"gkw\",\n",
        "    \"koeling\"\n",
        "]\n",
        "sublookup_ventilation_heating = [\n",
        "    \"verwarmingsblok\",\n",
        "    \"regelklep\",\n",
        "    \"CV\",\n",
        "    \"batterijen\",\n",
        "    \"verwarmingsbatterij\",\n",
        "    \"voorverwarmer\",\n",
        "    \"vorst\",\n",
        "    \"vorstgevaar\",\n",
        "    \"vorstmelding\",\n",
        "    \"vorst storing\",\n",
        "    \"vorstthermostaat\",\n",
        "    \"vorstbeveiliging\"\n",
        "]\n",
        "\n",
        "module_names = {\n",
        "    \"complaints\": \"Complaints\",\n",
        "    \"regularMaintenance\": \"Regular Maintenance\",\n",
        "    \"faultRedemption\": \"Fault Redemption\",\n",
        "    \"domesticWater\": \"Domestic Water\",\n",
        "    \"office\": \"Office\",\n",
        "    \"waterDistribution\": \"Water Distribution\",\n",
        "    \"bms\": \"BMS\",\n",
        "    \"wkk\": \"WKK\",\n",
        "    \"elevator\": \"Elevator\",\n",
        "    \"lighting\": \"Lighting\",\n",
        "    \"heatPump\": \"Heat Pump\",\n",
        "    \"sanitary\": \"Sanitary\",\n",
        "    \"fireSafety\": \"Fire Safety\",\n",
        "    \"shading\": \"Shading\",\n",
        "    \"entrance\": \"Entrance\",\n",
        "    \"ventilation\": \"Ventilation\",\n",
        "    \"heating\": \"Heating\",\n",
        "    \"cooling\": \"Cooling\",\n",
        "}\n",
        "\n",
        "submodule_names = {\n",
        "    \"regularMaintenance\": \"Regular Maintenance\",\n",
        "    \"bms\": \"BMS\",\n",
        "    \"heating\": \"Heating\",\n",
        "    \"cooling\": \"Cooling\",\n",
        "    \"undefinedMalfunctions\": \"Undefined Malfunctions\",\n",
        "    \"casing\": \"Casing\",\n",
        "    \"humidifier\": \"Humidifier\",\n",
        "    \"airFilter\": \"Air Filter\",\n",
        "    \"heatExchanger\": \"Heat Exchanger\",\n",
        "    \"fan\": \"Fan\",\n",
        "    \"damper\": \"Damper\"\n",
        "}\n",
        "\n",
        "normalize_lookups()\n",
        "\n",
        "# Read the lookup tables of subsystems and AHU components.\n",
        "list_lookups = [var for var in globals() if re.match(r'^lookup_', var)]\n",
        "list_sublookups = [var for var in globals() if re.match(r'^sublookup_', var)]\n",
        "\n",
        "# Sort all the lookup tables based on their lengths.\n",
        "list_lookups = sorted(list_lookups, key=lambda x: len(globals()[x]), reverse=True)\n",
        "list_sublookups = sorted(list_sublookups, key=lambda x: len(globals()[x]), reverse=True)\n",
        "\n",
        "# Move the lookup list of the unidentified service orders to the last position of the sorted list.\n",
        "list_sublookups.remove('sublookup_ventilation_undefinedMalfunctions')\n",
        "list_sublookups.append('sublookup_ventilation_undefinedMalfunctions')\n",
        "\n",
        "list_sublookups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFYjHZpCGigl"
      },
      "source": [
        "# Data Reading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aqqRwUyGqOe"
      },
      "outputs": [],
      "source": [
        "# Define the path to the dataset here.\n",
        "path = 'dataset_exel_file.xlsx'\n",
        "\n",
        "df_so = pd.read_excel(path)\n",
        "df_so.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Define the columns that you want to extract from the dataset here.\n",
        "columns_to_select = ['Description', 'Problem Text', 'Causation Text', 'Solution Text', 'SO_Orderdatum (Begindatum)', 'Order technisch gereed (Einddatum)', 'Factuurkosten SO']\n",
        "df_so = df_so[columns_to_select]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZRZ0oZgG3gB"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4Lz1Pbvl9iI"
      },
      "source": [
        "## Statistical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mx36kk6HtxD"
      },
      "outputs": [],
      "source": [
        "df_so.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmSykyXPHwJ1"
      },
      "outputs": [],
      "source": [
        "df_so.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDJL0LMGIC6r"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vANUeU_EIGs0"
      },
      "outputs": [],
      "source": [
        "textual_columns = ['Description', 'Problem Text', 'Causation Text', 'Solution Text']\n",
        "\n",
        "# Correct cell value types.\n",
        "df_so_cleaned = correct_types(df_so.copy(), textual_columns)\n",
        "\n",
        "# Replace punctuations.\n",
        "df_so_cleaned = replace_punctuation(df_so_cleaned, textual_columns)\n",
        "\n",
        "# Remove numerical values.\n",
        "df_so_cleaned = remove_numerical_values(df_so_cleaned, textual_columns)\n",
        "\n",
        "# Remove Dutch stop words.\n",
        "stop_words = set(stopwords.words('dutch'))\n",
        "stop_words.add('via')\n",
        "stop_words.remove('niet')\n",
        "df_so_cleaned[textual_columns] = df_so_cleaned[textual_columns].applymap(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
        "\n",
        "# Stemize the text.\n",
        "df_so_cleaned[textual_columns] = df_so_cleaned[textual_columns].applymap(stemize)\n",
        "\n",
        "df_so_cleaned.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9xAymHkIRnv"
      },
      "source": [
        "## Data Annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjkC1P61IUjs"
      },
      "outputs": [],
      "source": [
        "# Apply the 'group_modules' function to categorize each service order (SO) description into a main subsystem (module).\n",
        "# The 'target' column will store the assigned subsystem label for each service order based on the lookup tables.\n",
        "df_so_cleaned['target'] = df_so_cleaned['Description'].apply(lambda description: group_modules(str(description), list_lookups))\n",
        "\n",
        "# Filter the service orders to include only those categorized under the 'Ventilation' subsystem.\n",
        "df_so_cleaned_ventilation = df_so_cleaned[df_so_cleaned['target'] == 'Ventilation']\n",
        "\n",
        "# Apply the 'group_submodules' function to further categorize the ventilation-related service orders into submodules.\n",
        "# The 'submodule' column stores the assigned submodule label based on the 'list_sublookups' for ventilation (AHU components).\n",
        "df_so_cleaned['submodule'] = df_so_cleaned_ventilation['Description'].apply(lambda description: group_submodules(str(description), list_sublookups))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o18HZVcFGRF4"
      },
      "source": [
        "## Keep Original Description Column (Description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QAVZ-q3GaT_"
      },
      "outputs": [],
      "source": [
        "df_so_cleaned['original_sentence'] = df_so['Description']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi9-Fr-CunVf"
      },
      "source": [
        "## Get Ventilation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hjbTZKluu_9"
      },
      "outputs": [],
      "source": [
        "df_so_cleaned_ventilation = df_so_cleaned[df_so_cleaned['target'] == 'Ventilation']\n",
        "df_so_cleaned_ventilation.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VEXm_zEEivC"
      },
      "outputs": [],
      "source": [
        "df_so_cleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07xNgpNqLQzz"
      },
      "outputs": [],
      "source": [
        "# Calculate the counts for each target category.\n",
        "target_counts = df_so_cleaned[\"target\"].value_counts()\n",
        "\n",
        "# Calculate the total number of samples.\n",
        "total_samples = len(df_so_cleaned)\n",
        "\n",
        "# Plotting the bar chart.\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(target_counts.index, target_counts, color=plt.cm.viridis(np.linspace(0, 1, len(target_counts))))\n",
        "plt.title(\"Frequency of Target Categories\")\n",
        "plt.xlabel(\"Target Categories\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(minor=True)\n",
        "\n",
        "# Add counts on top of the bars.\n",
        "for bar, count in zip(bars, target_counts):\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, count + 1, str(count), ha='center', va='bottom')\n",
        "\n",
        "# Calculate and format percentages for legend labels.\n",
        "percentages = [(f\"{category} = {count/total_samples*100:.3f}%\") for category, count in target_counts.items()]\n",
        "\n",
        "# Add legend with custom labels.\n",
        "plt.legend(bars, percentages, loc='upper right', bbox_to_anchor=(1, 1), ncol=2, fontsize='small')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPa4E4eUKU3M"
      },
      "outputs": [],
      "source": [
        "# Calculate and print counts for each submodule category.\n",
        "submodule_counts = df_so_cleaned_ventilation[\"submodule\"].value_counts()\n",
        "total_samples = len(df_so_cleaned_ventilation)\n",
        "\n",
        "# Plotting the bar chart.\n",
        "plt.figure(figsize=(10, 7))\n",
        "bars = plt.bar(submodule_counts.index, submodule_counts, color=plt.cm.viridis(np.linspace(0, 1, len(submodule_counts))))\n",
        "plt.title(\"Frequency of AHU Subsystems' Service Orders\")\n",
        "plt.xlabel(\"AHU Subsystem\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(minor=True)\n",
        "\n",
        "# Add counts on top of the bars.\n",
        "for bar, count in zip(bars, submodule_counts):\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, count + 1, str(count), ha='center', va='bottom')\n",
        "\n",
        "# Calculate and format percentages for legend labels.\n",
        "percentages = [(f\"{category} = {count/total_samples*100:.2f}%\") for category, count in submodule_counts.items()]\n",
        "\n",
        "# Add legend with custom labels.\n",
        "plt.legend(bars, percentages, loc='upper right', bbox_to_anchor=(1, 1), ncol=2, fontsize='small')\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/Annotation (Classification)/ahu_component_categories.png', bbox_inches = 'tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcbwvqzJSJPX"
      },
      "source": [
        "# Visualization on Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNijDtcVOvBR"
      },
      "source": [
        "## Air Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4iQyesGlSTH"
      },
      "outputs": [],
      "source": [
        "# Extract service order descriptions related to the 'Air Filter' submodule from the cleaned ventilation DataFrame.\n",
        "corpus_airFilter = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Air Filter']['Description'].tolist()\n",
        "\n",
        "# Initialize CountVectorizer to create unigrams (1-grams).\n",
        "vectorizer_airFilter = CountVectorizer(ngram_range=(1, 1))\n",
        "\n",
        "# Fit the vectorizer on the air filter corpus and transform the text data into a document-term matrix (DTM).\n",
        "dtm_airFilter = vectorizer_airFilter.fit_transform(corpus_airFilter)\n",
        "\n",
        "# Get the feature names (unique terms) from the vectorizer.\n",
        "terms_airFilter = vectorizer_airFilter.get_feature_names_out()\n",
        "\n",
        "# Convert the DTM to a DataFrame for easier manipulation and analysis, using the terms as column names.\n",
        "dtm_airFilter = pd.DataFrame(dtm_airFilter.toarray(), columns=terms_airFilter)\n",
        "\n",
        "# Calculate the term frequencies by summing each column in the DTM.\n",
        "term_frequencies_airFilter = dtm_airFilter.sum()\n",
        "\n",
        "# Generate a word cloud from the term frequencies, specifying the desired size and background color.\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color='white').generate_from_frequencies(term_frequencies_airFilter)\n",
        "\n",
        "# Display the word cloud image without axis labels for a cleaner look.\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')  # Hide axis.\n",
        "# Save the word cloud image to the specified file path. Change the saving path in case of need!\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/word_clouds/ahu/air_filter.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKKlGoSmSPzY"
      },
      "outputs": [],
      "source": [
        "# Extract service order descriptions related to the 'Air Filter' submodule and split each description into individual words.\n",
        "transactions_airFilter = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Air Filter']['Description'].apply(lambda t: t.split(' '))\n",
        "\n",
        "# Convert the transactions to a list format.\n",
        "transactions_airFilter = list(transactions_airFilter)\n",
        "\n",
        "# Initialize the TransactionEncoder and fit it on the list of transactions.\n",
        "encoder_airFilter = TransactionEncoder().fit(transactions_airFilter)\n",
        "\n",
        "# Transform the transactions into a one-hot encoded format using the encoder.\n",
        "onehot_airFilter = encoder_airFilter.transform(transactions_airFilter)\n",
        "\n",
        "# Convert the one-hot encoded data into a DataFrame for easier manipulation and analysis, using the encoder's column names.\n",
        "onehot_airFilter = pd.DataFrame(onehot_airFilter, columns=encoder_airFilter.columns_)\n",
        "\n",
        "# Set the minimum occurrence threshold for itemsets.\n",
        "n = 3\n",
        "'''\n",
        "Define the minimum number of occurrences for each itemset.\n",
        "This indirectly acts as a hyperparamether, so tune it as you pefer.\n",
        "Increase the value of \"n\" if you want to find more frequent itemsets.\n",
        "Decrease the value of \"n\" if you want to find less frequent itemsets.\n",
        "'''\n",
        "# Calculate the minimum support threshold based on the total number of transactions.\n",
        "minimum_support = n / len(onehot_airFilter)\n",
        "\n",
        "# Apply the Apriori algorithm to find frequent itemsets in the one-hot encoded data with the specified minimum support.\n",
        "frequent_itemsets_airFilter = apriori(onehot_airFilter,\n",
        "                            min_support=minimum_support,\n",
        "                            use_colnames=True,\n",
        "                            verbose=1)\n",
        "\n",
        "# Generate association rules from the frequent itemsets using 'lift' as the metric.\n",
        "rules_airFilter = association_rules(frequent_itemsets_airFilter, metric='lift', min_threshold=1)\n",
        "\n",
        "# Print out key information regarding the transactions and rules derived.\n",
        "print(f'Number of transactions: {len(onehot_airFilter)}')\n",
        "print(f'Minimum number of occurrence for each itemset: {n}')\n",
        "print(f'Minimum support threshold for itemsets: {minimum_support}')\n",
        "print(f'Number of frequent itemsets: {len(frequent_itemsets_airFilter)}')\n",
        "print(f'Number of rules: {len(rules_airFilter)}')\n",
        "\n",
        "# Replace frozen sets in the 'antecedents' and 'consequents' columns with string representations for easier readability.\n",
        "rules_airFilter['antecedents'] = rules_airFilter['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_airFilter['consequents'] = rules_airFilter['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "# Transform the data into a pivot table format for heatmap visualization, indexing by consequents and columns by antecedents.\n",
        "pivot = rules_airFilter.pivot(index='consequents', columns='antecedents', values='support')\n",
        "\n",
        "# Create a mask to hide the upper triangle of the heatmap for better visual clarity.\n",
        "mask = np.triu(np.ones_like(pivot, dtype=bool))\n",
        "\n",
        "# Set the font scale for the seaborn heatmap.\n",
        "sns.set(font_scale=0.9)\n",
        "\n",
        "# Create a figure for the heatmap with specified dimensions.\n",
        "plt.figure(figsize=(15, 8))  # Adjust width and height as needed.\n",
        "\n",
        "# Generate the heatmap using seaborn, applying the mask, color map, and other visual settings.\n",
        "heatmap = sns.heatmap(pivot, cmap='coolwarm', mask=mask, cbar=True, linewidths=0.5, linecolor='black', annot=True, fmt='.2f')\n",
        "\n",
        "# Rotate the x-axis labels for better visibility.\n",
        "plt.xticks(rotation=45)\n",
        "# Set the title for the heatmap, including transaction count and thresholds.\n",
        "plt.title(f'Air Filter Rules Heatmap: {len(onehot_airFilter)} transactions, occurrence min-value of {n}, minimum support threshold of {minimum_support:.2f}')\n",
        "# Rotate the y-axis labels for better visibility.\n",
        "plt.yticks(rotation=0)\n",
        "# Display the heatmap.\n",
        "plt.show()\n",
        "# Save the generated heatmap image to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/heatmaps/ahu/air_filter.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZinGAXGbHdL"
      },
      "outputs": [],
      "source": [
        "# Sort the rules based on support in descending order to prioritize the most frequent itemsets.\n",
        "rules_airFilter.sort_values(by='support', ascending=False, inplace=True)\n",
        "\n",
        "# Create a new column 'itemset' that combines the antecedents and consequents into a single string representation.\n",
        "rules_airFilter['itemset'] = rules_airFilter.apply(lambda row: f\"{row['antecedents']},{row['consequents']}\", axis=1)\n",
        "\n",
        "# Calculate the count for each rule based on its support and the total number of transactions.\n",
        "rules_airFilter['count'] = rules_airFilter.apply(lambda row: int(row['support'] * len(onehot_airFilter)), axis=1)\n",
        "\n",
        "# Convert the string representation of the itemset back into a set for better data structure handling.\n",
        "rules_airFilter['itemset'] = rules_airFilter['itemset'].apply(convert_str_to_set)\n",
        "\n",
        "# Round the support values to three decimal places for cleaner display.\n",
        "rules_airFilter['rounded_support'] = rules_airFilter.apply(lambda row: f\"{row['support']:.3f}\", axis=1)\n",
        "\n",
        "# Calculate the percentage of occurrences for each itemset relative to the total number of transactions.\n",
        "rules_airFilter['percentage'] = rules_airFilter.apply(lambda row: f\"{row['count'] * 100 / len(onehot_airFilter):.2f}%\", axis=1)\n",
        "\n",
        "# Remove duplicate itemsets, keeping only the first occurrence to avoid redundancy in the final output.\n",
        "rules_airFilter.drop_duplicates(subset=['itemset'], keep='first', inplace=True)\n",
        "\n",
        "# Reset the index of the DataFrame for tidy representation.\n",
        "rules_airFilter.reset_index(inplace=True)\n",
        "\n",
        "# Select relevant columns to display in the final output.\n",
        "rules_airFilter[['itemset', 'support', 'count', 'percentage']]\n",
        "\n",
        "# Create a figure for displaying the results in a table format.\n",
        "fig, ax = plt.subplots(figsize=(10, 3))\n",
        "ax.axis('tight')  # Fit the axes tightly around the table.\n",
        "ax.axis('off')    # Hide the axes for a cleaner appearance.\n",
        "\n",
        "# Create a table in the plot using the selected columns from the rules DataFrame.\n",
        "ax.table(cellText=rules_airFilter[['itemset', 'rounded_support', 'count', 'percentage']].values,\n",
        "         colLabels=rules_airFilter[['itemset', 'support', 'count', 'percentage']].columns,\n",
        "         cellLoc='center', loc='center', fontsize=12)\n",
        "\n",
        "# Set the title for the table figure.\n",
        "plt.title('Air Filter Rules')\n",
        "\n",
        "# Adjust the layout for optimal display.\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the generated table image to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/tables/ahu/air_filter.png')\n",
        "\n",
        "# Display the table.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iysRr1zOO2e1"
      },
      "source": [
        "## Fan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcTxerrvoKWY"
      },
      "outputs": [],
      "source": [
        "# Extract the service order descriptions related to the 'Fan' submodule from the cleaned ventilation DataFrame.\n",
        "corpus_fan = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Fan']['Description'].tolist()\n",
        "\n",
        "# Initialize the CountVectorizer to create a bag-of-words model; ngram_range=(1, 1) means we are using single words (unigrams).\n",
        "vectorizer_fan = CountVectorizer(ngram_range=(1, 1))\n",
        "\n",
        "# Fit the CountVectorizer on the corpus and transform it into a Document-Term Matrix (DTM).\n",
        "dtm_fan = vectorizer_fan.fit_transform(corpus_fan)\n",
        "\n",
        "# Retrieve the feature names (terms) from the vectorizer.\n",
        "terms_fan = vectorizer_fan.get_feature_names_out()\n",
        "\n",
        "# Convert the sparse matrix representation of the DTM to a dense format and create a DataFrame for easier manipulation.\n",
        "dtm_fan = pd.DataFrame(dtm_fan.toarray(), columns=terms_fan)\n",
        "\n",
        "# Sum the term frequencies across all documents in the DTM.\n",
        "term_frequencies_fan = dtm_fan.sum()\n",
        "\n",
        "# Generate a word cloud from the term frequencies, setting the image size and background color.\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color='white').generate_from_frequencies(term_frequencies_fan)\n",
        "\n",
        "# Optionally set the title for the word cloud figure (currently commented out).\n",
        "# plt.title('Fan Word Cloud')\n",
        "\n",
        "# Create a figure to display the word cloud, adjusting the size for better visibility.\n",
        "# plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Display the word cloud image.\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "\n",
        "# Hide the axes for a cleaner appearance of the word cloud.\n",
        "plt.axis('off')\n",
        "\n",
        "# Save the generated word cloud image to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/word_clouds/ahu/fan.png')\n",
        "\n",
        "# Show the word cloud plot.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFsdmd0JUSZ8"
      },
      "outputs": [],
      "source": [
        "# Extract service order descriptions related to the 'Fan' submodule from the cleaned ventilation DataFrame and split the text into individual words.\n",
        "transactions_fan = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Fan']['Description'].apply(lambda t: t.split(' '))\n",
        "\n",
        "# Convert the resulting Series of lists into a list of transactions.\n",
        "transactions_fan = list(transactions_fan)\n",
        "\n",
        "# Initialize the TransactionEncoder to encode the list of transactions into a one-hot encoded format.\n",
        "encoder_fan = TransactionEncoder().fit(transactions_fan)\n",
        "\n",
        "# Transform the transactions into a one-hot encoded DataFrame format.\n",
        "onehot_fan = encoder_fan.transform(transactions_fan)\n",
        "\n",
        "# Create a DataFrame from the one-hot encoded data, using the encoder's column names.\n",
        "onehot_fan = pd.DataFrame(onehot_fan, columns=encoder_fan.columns_)\n",
        "\n",
        "# Set the minimum occurrence threshold for itemsets.\n",
        "'''\n",
        "Define the minimum number of occurrences for each itemset.\n",
        "This indirectly acts as a hyperparamether, so tune it as you pefer.\n",
        "Increase the value of \"n\" if you want to find more frequent itemsets.\n",
        "Decrease the value of \"n\" if you want to find less frequent itemsets.\n",
        "'''\n",
        "n = 3\n",
        "\n",
        "# Calculate the minimum support threshold based on the total number of transactions.\n",
        "minimum_support = n / len(onehot_fan)\n",
        "\n",
        "# Apply the Apriori algorithm to find frequent itemsets in the one-hot encoded DataFrame.\n",
        "frequent_itemsets_fan = apriori(onehot_fan,\n",
        "                            min_support=minimum_support,\n",
        "                            use_colnames=True,\n",
        "                            verbose=1)\n",
        "\n",
        "# Generate association rules from the frequent itemsets, using 'lift' as the evaluation metric.\n",
        "rules_fan = association_rules(frequent_itemsets_fan, metric='lift', min_threshold=1)\n",
        "\n",
        "# Print summary information about the transactions and rules generated.\n",
        "print(f'Number of transactions: {len(onehot_fan)}')  # Total number of transactions\n",
        "print(f'Minimum number of occurrences for each itemset: {n}')  # Minimum itemset count\n",
        "print(f'Minimum support threshold for itemsets: {minimum_support}')  # Minimum support threshold\n",
        "print(f'Number of frequent itemsets: {len(frequent_itemsets_fan)}')  # Total number of frequent itemsets\n",
        "print(f'Number of rules: {len(rules_fan)}')  # Total number of association rules\n",
        "\n",
        "# Replace frozen sets in the rules with string representations for easier interpretation.\n",
        "rules_fan['antecedents'] = rules_fan['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_fan['consequents'] = rules_fan['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "# Transform the rules DataFrame into a pivot table format for heatmap generation.\n",
        "pivot = rules_fan.pivot(index='consequents', columns='antecedents', values='support')\n",
        "\n",
        "# Create a mask to display only the lower triangle of the heatmap.\n",
        "mask = np.triu(np.ones_like(pivot, dtype=bool))\n",
        "\n",
        "# Set the font scale for the seaborn heatmap.\n",
        "sns.set(font_scale=0.9)\n",
        "\n",
        "# Create a new figure for the heatmap, adjusting the size for clarity.\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Generate the heatmap using the pivot table, with color map and annotations.\n",
        "heatmap = sns.heatmap(pivot, cmap='coolwarm', mask=mask, cbar=True, linewidths=0.5, linecolor='black', annot=True, fmt='.2f')\n",
        "\n",
        "# Rotate x-axis tick labels for better readability.\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Set the title for the heatmap, including relevant details about transactions and thresholds.\n",
        "plt.title(f'Fan Rules Heatmap: {len(onehot_fan)} transactions, occurrence min-value of {n}, minimum support threshold of {minimum_support:.2f}')\n",
        "\n",
        "# Rotate y-axis tick labels for better readability.\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Save the generated heatmap image to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/heatmaps/ahu/fan.png')\n",
        "\n",
        "# Show the generated heatmap plot.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHodesY6QfNK"
      },
      "outputs": [],
      "source": [
        "# Sort the rules based on support in descending order for better visualization of the most frequent itemsets.\n",
        "rules_fan.sort_values(by='support', ascending=False, inplace=True)\n",
        "\n",
        "# Create a new 'itemset' column by concatenating the 'antecedents' and 'consequents' columns.\n",
        "rules_fan['itemset'] = rules_fan.apply(lambda row: f\"{row['antecedents']},{row['consequents']}\", axis=1)\n",
        "\n",
        "# Calculate the count of occurrences for each rule by multiplying the support by the total number of transactions.\n",
        "rules_fan['count'] = rules_fan.apply(lambda row: int(row['support'] * len(onehot_fan)), axis=1)\n",
        "\n",
        "# Convert the 'itemset' strings into sets for easier manipulation and analysis.\n",
        "rules_fan['itemset'] = rules_fan['itemset'].apply(convert_str_to_set)\n",
        "\n",
        "# Round the support values to three decimal places for clearer presentation.\n",
        "rules_fan['rounded_support'] = rules_fan.apply(lambda row: f\"{row['support']:.3f}\", axis=1)\n",
        "\n",
        "# Calculate the percentage representation of each rule's count relative to the total number of transactions.\n",
        "rules_fan['percentage'] = rules_fan.apply(lambda row: f\"{row['count'] * 100 / len(onehot_fan):.2f}%\", axis=1)\n",
        "\n",
        "# Remove duplicate itemsets, keeping only the first occurrence to avoid redundancy.\n",
        "rules_fan.drop_duplicates(subset=['itemset'], keep='first', inplace=True)\n",
        "\n",
        "# Reset the index of the DataFrame after dropping duplicates for better readability and organization.\n",
        "rules_fan.reset_index(inplace=True)\n",
        "\n",
        "# Select the relevant columns for display.\n",
        "rules_fan[['itemset', 'support', 'count', 'percentage']]\n",
        "\n",
        "# Create a figure for the table visualization.\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "\n",
        "# Set the axes to be tight and turn off the axis display for a cleaner look.\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "# Create a table using the selected columns from the rules DataFrame.\n",
        "ax.table(cellText=rules_fan[['itemset', 'rounded_support', 'count', 'percentage']].values,\n",
        "         colLabels=rules_fan[['itemset', 'support', 'count', 'percentage']].columns,\n",
        "         cellLoc='center', loc='center', fontsize=12)\n",
        "\n",
        "# Set the title for the table visualization.\n",
        "plt.title('Fan Rules')\n",
        "\n",
        "# Adjust the layout to prevent overlap and ensure everything fits nicely.\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the generated table image to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/tables/ahu/fan.png')\n",
        "\n",
        "# Display the generated table plot.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXDPFv0tRGjm"
      },
      "source": [
        "##  Regular Maintenance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEW3fYwpojLN"
      },
      "outputs": [],
      "source": [
        "# Extract the descriptions of service orders related to 'Regular Maintenance' in the ventilation subsystem.\n",
        "corpus_regularMaintenance = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Regular Maintenance']['Description'].tolist()\n",
        "\n",
        "# Initialize the CountVectorizer to create a document-term matrix with unigrams (1-grams).\n",
        "vectorizer_regularMaintenance = CountVectorizer(ngram_range=(1, 1))\n",
        "\n",
        "# Fit the vectorizer on the corpus and transform the text data into a document-term matrix (DTM).\n",
        "dtm_regularMaintenance = vectorizer_regularMaintenance.fit_transform(corpus_regularMaintenance)\n",
        "\n",
        "# Retrieve the feature names (unique terms) from the vectorizer.\n",
        "terms_regularMaintenance = vectorizer_regularMaintenance.get_feature_names_out()\n",
        "\n",
        "# Convert the DTM to a DataFrame for easier manipulation and analysis.\n",
        "dtm_regularMaintenance = pd.DataFrame(dtm_regularMaintenance.toarray(), columns=terms_regularMaintenance)\n",
        "\n",
        "# Sum the occurrences of each term to get term frequencies.\n",
        "term_frequencies_regularMaintenance = dtm_regularMaintenance.sum()\n",
        "\n",
        "# Generate a word cloud image based on the term frequencies, with specified dimensions and background color.\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color='white').generate_from_frequencies(term_frequencies_regularMaintenance)\n",
        "\n",
        "# The following two lines are commented out. They can be used to set the title and size for displaying the word cloud.\n",
        "# plt.title('Regular Maintenance Word Cloud')\n",
        "# plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Display the word cloud image.\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "\n",
        "# Turn off the axis for a cleaner visualization.\n",
        "plt.axis('off')\n",
        "\n",
        "# Save the generated word cloud image to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/word_clouds/ahu/regular_maintenance.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJR2EIqIVGam"
      },
      "outputs": [],
      "source": [
        "# Extract the descriptions of service orders related to 'Regular Maintenance' in the ventilation subsystem,\n",
        "# splitting each description into a list of words (tokens).\n",
        "transactions_regularMaintenance = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Regular Maintenance']['Description'].apply(lambda t: t.split(' '))\n",
        "\n",
        "# Convert the series of transactions into a list for further processing.\n",
        "transactions_regularMaintenance = list(transactions_regularMaintenance)\n",
        "\n",
        "# Initialize the TransactionEncoder to prepare the data for one-hot encoding.\n",
        "encoder_regularMaintenance = TransactionEncoder().fit(transactions_regularMaintenance)\n",
        "\n",
        "# Transform the list of transactions into a one-hot encoded format.\n",
        "onehot_regularMaintenance = encoder_regularMaintenance.transform(transactions_regularMaintenance)\n",
        "\n",
        "# Convert the one-hot encoded data into a DataFrame for easier analysis and manipulation.\n",
        "onehot_regularMaintenance = pd.DataFrame(onehot_regularMaintenance, columns=encoder_regularMaintenance.columns_)\n",
        "\n",
        "# Set the minimum occurrence threshold for itemsets.\n",
        "'''\n",
        "Define the minimum number of occurrences for each itemset.\n",
        "This indirectly acts as a hyperparamether, so tune it as you pefer.\n",
        "Increase the value of \"n\" if you want to find more frequent itemsets.\n",
        "Decrease the value of \"n\" if you want to find less frequent itemsets.\n",
        "'''\n",
        "n = 1\n",
        "\n",
        "# Calculate the minimum support threshold based on the number of transactions.\n",
        "minimum_support = n / len(onehot_regularMaintenance)\n",
        "\n",
        "# Apply the apriori algorithm to find frequent itemsets in the one-hot encoded data,\n",
        "# using the minimum support threshold.\n",
        "frequent_itemsets_regularMaintenance = apriori(onehot_regularMaintenance,\n",
        "                            min_support=minimum_support,\n",
        "                            use_colnames=True,\n",
        "                            verbose=1)\n",
        "\n",
        "# Generate association rules from the frequent itemsets, using lift as the evaluation metric.\n",
        "rules_regularMaintenance = association_rules(frequent_itemsets_regularMaintenance, metric='lift', min_threshold=1)\n",
        "\n",
        "# Print the analysis results for debugging and information.\n",
        "print(f'Number of transactions: {len(onehot_regularMaintenance)}')  # Total transactions\n",
        "print(f'Minimum number of occurrence for each itemset: {n}')  # Minimum occurrence\n",
        "print(f'Minimum support threshold for itemsets: {minimum_support}')  # Minimum support threshold\n",
        "print(f'Number of frequent itemsets: {len(frequent_itemsets_regularMaintenance)}')  # Frequent itemsets count\n",
        "print(f'Number of rules: {len(rules_regularMaintenance)}')  # Total rules generated\n",
        "\n",
        "# Replace frozen sets in the antecedents with strings for better readability.\n",
        "rules_regularMaintenance['antecedents'] = rules_regularMaintenance['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "# Similarly, replace frozen sets in the consequents with strings.\n",
        "rules_regularMaintenance['consequents'] = rules_regularMaintenance['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "# Transform the rules data into a matrix format suitable for heatmap visualization.\n",
        "pivot = rules_regularMaintenance.pivot(index='consequents', columns='antecedents', values='support')\n",
        "\n",
        "# Create a mask for the upper triangle of the heatmap to enhance visualization.\n",
        "mask = np.triu(np.ones_like(pivot, dtype=bool))\n",
        "\n",
        "# Set the font scale for the seaborn plots.\n",
        "sns.set(font_scale=0.9)  # Adjust the font size as needed\n",
        "\n",
        "# Create a figure for the heatmap with specified dimensions.\n",
        "plt.figure(figsize=(15, 8))  # Adjust width and height as needed\n",
        "\n",
        "# Generate the heatmap for the association rules.\n",
        "heatmap = sns.heatmap(pivot, cmap='coolwarm', mask=mask, cbar=True, linewidths=0.5, linecolor='black', annot=True, fmt='.2f')\n",
        "\n",
        "# Rotate the x-ticks for better visibility.\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Set the title of the heatmap with dynamic values for transactions and thresholds.\n",
        "plt.title(f'Regular Maintenance Rules Heatmap: {len(onehot_regularMaintenance)} transactions, occurrence min-value of {n}, minimum support threshold of {minimum_support:.2f}')\n",
        "\n",
        "# Rotate the y-ticks for better visibility.\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Save the generated heatmap image to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/heatmaps/ahu/regular_maintenance.png')\n",
        "\n",
        "# Display the heatmap.\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDk3xoURRs16"
      },
      "outputs": [],
      "source": [
        "# Sort the rules by support in descending order to prioritize higher support values.\n",
        "rules_regularMaintenance.sort_values(by='support', ascending=False, inplace=True)\n",
        "\n",
        "# Create a new column 'itemset' that combines the antecedents and consequents into a single string representation.\n",
        "rules_regularMaintenance['itemset'] = rules_regularMaintenance.apply(lambda row: f\"{row['antecedents']},{row['consequents']}\", axis=1)\n",
        "\n",
        "# Calculate the count of occurrences for each rule based on the support and total number of transactions.\n",
        "rules_regularMaintenance['count'] = rules_regularMaintenance.apply(lambda row: int(row['support'] * len(onehot_regularMaintenance)), axis=1)\n",
        "\n",
        "# Convert the 'itemset' strings into sets for easier analysis and comparison.\n",
        "rules_regularMaintenance['itemset'] = rules_regularMaintenance['itemset'].apply(convert_str_to_set)\n",
        "\n",
        "# Round the support values to three decimal places for better readability.\n",
        "rules_regularMaintenance['rounded_support'] = rules_regularMaintenance.apply(lambda row: f\"{row['support']:.3f}\", axis=1)\n",
        "\n",
        "# Calculate the percentage representation of each count relative to the total number of transactions.\n",
        "rules_regularMaintenance['percentage'] = rules_regularMaintenance.apply(lambda row: f\"{row['count'] * 100 / len(onehot_regularMaintenance):.2f}%\", axis=1)\n",
        "\n",
        "# Remove duplicate itemsets, keeping only the first occurrence to ensure unique rules.\n",
        "rules_regularMaintenance.drop_duplicates(subset=['itemset'], keep='first', inplace=True)\n",
        "\n",
        "# Reset the index of the DataFrame for cleaner output after modifications.\n",
        "rules_regularMaintenance.reset_index(inplace=True)\n",
        "\n",
        "# Display the relevant columns for inspection.\n",
        "rules_regularMaintenance[['itemset', 'support', 'count', 'percentage']]\n",
        "\n",
        "# Create a figure for the table display with specified dimensions.\n",
        "fig, ax = plt.subplots(figsize=(10, 3))\n",
        "\n",
        "# Set axes to tight and turn off the axes for a cleaner table display.\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "# Generate a table with the specified columns and format it for better visibility.\n",
        "ax.table(cellText=rules_regularMaintenance[['itemset', 'rounded_support', 'count', 'percentage']].values,\n",
        "         colLabels=rules_regularMaintenance[['itemset', 'support', 'count', 'percentage']].columns,\n",
        "         cellLoc='center', loc='center', fontsize=12)\n",
        "\n",
        "# Set the title of the table for clarity.\n",
        "plt.title('Regular Maintenance Rules')\n",
        "\n",
        "# Adjust the layout to fit everything neatly in the figure.\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the generated table image to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/tables/ahu/regular_maintenance.png')\n",
        "\n",
        "# Display the table.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJwYgypSSYR4"
      },
      "source": [
        "## BMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeYQHzbRpCra"
      },
      "outputs": [],
      "source": [
        "# Extract descriptions of service orders related to the BMS (Building Management System) submodule.\n",
        "corpus_bms = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'BMS']['Description'].tolist()\n",
        "\n",
        "# Initialize CountVectorizer to convert text data into a matrix of token counts (1-grams).\n",
        "vectorizer_bms = CountVectorizer(ngram_range=(1, 1))\n",
        "\n",
        "# Fit the vectorizer to the BMS corpus and transform the text into a Document-Term Matrix (DTM).\n",
        "dtm_bms = vectorizer_bms.fit_transform(corpus_bms)\n",
        "\n",
        "# Get the feature names (terms) from the vectorizer.\n",
        "terms_bms = vectorizer_bms.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame from the DTM, using the terms as column names.\n",
        "dtm_bms = pd.DataFrame(dtm_bms.toarray(), columns=terms_bms)\n",
        "\n",
        "# Sum the occurrences of each term to get the term frequencies.\n",
        "term_frequencies_bms = dtm_bms.sum()\n",
        "\n",
        "# Generate a word cloud image based on the term frequencies.\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color='white').generate_from_frequencies(term_frequencies_bms)\n",
        "\n",
        "# Optional: Set title and size for the figure if displaying the word cloud.\n",
        "# plt.title('BMS Word Cloud')\n",
        "# plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Display the generated word cloud image.\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "\n",
        "# Hide the axes for a cleaner look.\n",
        "plt.axis('off')\n",
        "\n",
        "# Save the word cloud image to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/word_clouds/ahu/bms.png')\n",
        "\n",
        "# Show the word cloud on the screen.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl7NzTgnWLzc"
      },
      "outputs": [],
      "source": [
        "# Extract service order descriptions related to the BMS (Building Management System) submodule.\n",
        "transactions_bms = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'BMS']['Description'].apply(lambda t: t.split(' '))\n",
        "\n",
        "# Convert the transactions series into a list of transactions.\n",
        "transactions_bms = list(transactions_bms)\n",
        "\n",
        "# Initialize the TransactionEncoder, which is used to convert transactions into a one-hot encoded format.\n",
        "encoder_bms = TransactionEncoder().fit(transactions_bms)\n",
        "\n",
        "# Transform the transactions into a one-hot encoded DataFrame.\n",
        "onehot_bms = encoder_bms.transform(transactions_bms)\n",
        "\n",
        "# Create a DataFrame from the one-hot encoded data, using the encoder's column names.\n",
        "onehot_bms = pd.DataFrame(onehot_bms, columns=encoder_bms.columns_)\n",
        "\n",
        "# Set the minimum number of occurrences required for each itemset.\n",
        "'''\n",
        "Define the minimum number of occurrences for each itemset.\n",
        "This indirectly acts as a hyperparamether, so tune it as you pefer.\n",
        "Increase the value of \"n\" if you want to find more frequent itemsets.\n",
        "Decrease the value of \"n\" if you want to find less frequent itemsets.\n",
        "'''\n",
        "n = 2\n",
        "\n",
        "# Calculate the minimum support threshold based on the total number of transactions.\n",
        "minimum_support = n / len(onehot_bms)\n",
        "\n",
        "# Apply the Apriori algorithm to find frequent itemsets with the specified minimum support.\n",
        "frequent_itemsets_bms = apriori(onehot_bms,\n",
        "                                 min_support=minimum_support,\n",
        "                                 use_colnames=True,\n",
        "                                 verbose=1)\n",
        "\n",
        "# Generate association rules from the frequent itemsets using the lift metric.\n",
        "rules_bms = association_rules(frequent_itemsets_bms, metric='lift', min_threshold=1)\n",
        "\n",
        "# Print key metrics about the transaction data and association rules generated.\n",
        "print(f'Number of transactions: {len(onehot_bms)}')\n",
        "print(f'Minimum number of occurrences for each itemset: {n}')\n",
        "print(f'Minimum support threshold for itemsets: {minimum_support}')\n",
        "print(f'Number of frequent itemsets: {len(frequent_itemsets_bms)}')\n",
        "print(f'Number of rules: {len(rules_bms)}')\n",
        "\n",
        "# Replace the frozen sets in the 'antecedents' and 'consequents' columns with string representations.\n",
        "rules_bms['antecedents'] = rules_bms['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_bms['consequents'] = rules_bms['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "# Transform the rules data into a pivot format suitable for creating a heatmap.\n",
        "pivot = rules_bms.pivot(index='consequents', columns='antecedents', values='support')\n",
        "\n",
        "# Create a mask to only show the lower triangle of the heatmap.\n",
        "mask = np.triu(np.ones_like(pivot, dtype=bool))\n",
        "\n",
        "# Set font size for the heatmap visualization.\n",
        "sns.set(font_scale=0.9)\n",
        "\n",
        "# Create a figure for the heatmap with specified dimensions.\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Generate the heatmap using seaborn, customizing the color palette and annotations.\n",
        "heatmap = sns.heatmap(pivot, cmap='coolwarm', cbar=True, linewidths=0.5, linecolor='black', mask=mask, annot=True, fmt='.2f')\n",
        "\n",
        "# Rotate x-axis labels for better readability.\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Set the title for the heatmap, including transaction and threshold information.\n",
        "plt.title(f'BMS Rules Heatmap: {len(onehot_bms)} transactions, occurrence min-value of {n}, minimum support threshold of {minimum_support:.2f}')\n",
        "\n",
        "# Rotate y-axis labels for better readability.\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Save the heatmap figure to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/heatmaps/ahu/bms.png')\n",
        "\n",
        "# Display the heatmap on the screen.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N51x8_FhSuuW"
      },
      "outputs": [],
      "source": [
        "# Sort the rules by support in descending order to prioritize the strongest associations.\n",
        "rules_bms.sort_values(by='support', ascending=False, inplace=True)\n",
        "\n",
        "# Create a new column 'itemset' that combines the 'antecedents' and 'consequents' into a single string representation.\n",
        "rules_bms['itemset'] = rules_bms.apply(lambda row: f\"{row['antecedents']},{row['consequents']}\", axis=1)\n",
        "\n",
        "# Calculate the count of occurrences for each itemset based on its support and the total number of transactions.\n",
        "rules_bms['count'] = rules_bms.apply(lambda row: int(row['support'] * len(onehot_bms)), axis=1)\n",
        "\n",
        "# Convert the 'itemset' strings back into sets for consistency and further analysis.\n",
        "rules_bms['itemset'] = rules_bms['itemset'].apply(convert_str_to_set)\n",
        "\n",
        "# Round the support values to three decimal places for cleaner presentation.\n",
        "rules_bms['rounded_support'] = rules_bms.apply(lambda row: f\"{row['support']:.3f}\", axis=1)\n",
        "\n",
        "# Calculate the percentage of occurrences for each itemset relative to the total number of transactions.\n",
        "rules_bms['percentage'] = rules_bms.apply(lambda row: f\"{row['count'] * 100 / len(onehot_bms):.2f}%\", axis=1)\n",
        "\n",
        "# Remove duplicate itemsets, keeping only the first occurrence.\n",
        "rules_bms.drop_duplicates(subset=['itemset'], keep='first', inplace=True)\n",
        "\n",
        "# Reset the index of the DataFrame after filtering duplicates.\n",
        "rules_bms.reset_index(inplace=True)\n",
        "\n",
        "# Display the relevant columns for the itemsets, support, count, and percentage.\n",
        "rules_bms[['itemset', 'support', 'count', 'percentage']]\n",
        "\n",
        "# Create a figure for displaying the rules in table format.\n",
        "fig, ax = plt.subplots(figsize=(10, 3))\n",
        "\n",
        "# Set the axes to tight layout and turn off axis display since this is a table.\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "# Create a table with the specified columns from the rules_bms DataFrame.\n",
        "ax.table(cellText=rules_bms[['itemset', 'rounded_support', 'count', 'percentage']].values,\n",
        "         colLabels=rules_bms[['itemset', 'support', 'count', 'percentage']].columns,\n",
        "         cellLoc='center', loc='center', fontsize=12)\n",
        "\n",
        "# Set the title for the table visualization.\n",
        "plt.title('BMS Rules')\n",
        "\n",
        "# Adjust layout to prevent overlapping of elements.\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure to the specified file path for later use.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/tables/ahu/bms.png')\n",
        "\n",
        "# Display the table on the screen.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1urK-tHTDkv"
      },
      "source": [
        "## Heating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqJ4cEp8pdjn"
      },
      "outputs": [],
      "source": [
        "# Extract the service order descriptions related to the 'Heating' submodule into a list.\n",
        "corpus_heating = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Heating']['Description'].tolist()\n",
        "\n",
        "# Initialize the CountVectorizer to convert the text data into a matrix of token counts (unigrams).\n",
        "vectorizer_heating = CountVectorizer(ngram_range=(1, 1))\n",
        "\n",
        "# Fit the CountVectorizer on the corpus and transform the data into a Document-Term Matrix (DTM).\n",
        "dtm_heating = vectorizer_heating.fit_transform(corpus_heating)\n",
        "\n",
        "# Get the feature names (terms) from the vectorizer.\n",
        "terms_heating = vectorizer_heating.get_feature_names_out()\n",
        "\n",
        "# Convert the sparse matrix to a DataFrame for easier manipulation and analysis.\n",
        "dtm_heating = pd.DataFrame(dtm_heating.toarray(), columns=terms_heating)\n",
        "\n",
        "# Sum the occurrences of each term to get the term frequencies.\n",
        "term_frequencies_heating = dtm_heating.sum()\n",
        "\n",
        "# Generate a word cloud image from the term frequencies, customizing the appearance.\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color='white').generate_from_frequencies(term_frequencies_heating)\n",
        "\n",
        "# Uncomment the next line to set a title for the word cloud visualization (if desired).\n",
        "# plt.title('Heating-Sub Word Cloud')\n",
        "\n",
        "# Create a figure for displaying the word cloud.\n",
        "# Uncomment the next line to specify the size of the figure (if desired).\n",
        "# plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Display the generated word cloud image using bilinear interpolation for smooth rendering.\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "\n",
        "# Turn off the axis display for a cleaner look.\n",
        "plt.axis('off')\n",
        "\n",
        "# Save the word cloud image to the specified file path for later use.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/word_clouds/ahu/heating.png')\n",
        "\n",
        "# Display the word cloud on the screen.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlU5P7Up_fVb"
      },
      "outputs": [],
      "source": [
        "# Extract the service order descriptions related to the 'Heating' submodule and split each description into a list of words.\n",
        "transactions_heating = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Heating']['Description'].apply(lambda t: t.split(' '))\n",
        "# Convert the series of lists into a single list of transactions.\n",
        "transactions_heating = list(transactions_heating)\n",
        "\n",
        "# Initialize the TransactionEncoder to convert the transaction lists into a one-hot encoded format.\n",
        "encoder_heating = TransactionEncoder().fit(transactions_heating)\n",
        "# Transform the list of transactions into a one-hot encoded DataFrame.\n",
        "onehot_heating = encoder_heating.transform(transactions_heating)\n",
        "# Create a DataFrame from the one-hot encoded array for easier manipulation and analysis.\n",
        "onehot_heating = pd.DataFrame(onehot_heating, columns=encoder_heating.columns_)\n",
        "\n",
        "# Set the minimum number of occurrences for each itemset to 2.\n",
        "'''\n",
        "Define the minimum number of occurrences for each itemset.\n",
        "This indirectly acts as a hyperparamether, so tune it as you pefer.\n",
        "Increase the value of \"n\" if you want to find more frequent itemsets.\n",
        "Decrease the value of \"n\" if you want to find less frequent itemsets.\n",
        "'''\n",
        "n = 2\n",
        "\n",
        "# Calculate the minimum support threshold based on the number of transactions.\n",
        "minimum_support = n / len(onehot_heating)\n",
        "\n",
        "# Apply the Apriori algorithm to find frequent itemsets with the specified minimum support.\n",
        "frequent_itemsets_heating = apriori(onehot_heating,\n",
        "                                     min_support=minimum_support,\n",
        "                                     use_colnames=True,\n",
        "                                     verbose=1)\n",
        "\n",
        "# Generate association rules from the frequent itemsets using lift as the metric.\n",
        "rules_heating = association_rules(frequent_itemsets_heating, metric='lift', min_threshold=1)\n",
        "\n",
        "# Print the number of transactions and key parameters for analysis.\n",
        "print(f'Number of transactions: {len(onehot_heating)}')\n",
        "print(f'Minimum number of occurrence for each itemset: {n}')\n",
        "print(f'Minimum support threshold for itemsets: {minimum_support}')\n",
        "print(f'Number of frequent itemsets: {len(frequent_itemsets_heating)}')\n",
        "print(f'Number of rules: {len(rules_heating)}')\n",
        "\n",
        "# Replace the frozen sets in the rules with strings for easier readability.\n",
        "rules_heating['antecedents'] = rules_heating['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_heating['consequents'] = rules_heating['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "# Transform the rules DataFrame to a matrix format suitable for generating a heatmap.\n",
        "pivot = rules_heating.pivot(index='consequents', columns='antecedents', values='support')\n",
        "\n",
        "# Create a mask to hide the upper triangle of the heatmap.\n",
        "mask = np.triu(np.ones_like(pivot, dtype=bool))\n",
        "\n",
        "# Set the font scale for the heatmap visualization.\n",
        "sns.set(font_scale=0.9)\n",
        "# Create a new figure for the heatmap with specified dimensions.\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Generate the heatmap with the specified color map and formatting options.\n",
        "heatmap = sns.heatmap(pivot, cmap='coolwarm', cbar=True, linewidths=0.5, linecolor='black', mask=mask, annot=True, fmt='.2f')\n",
        "\n",
        "# Rotate the x-axis tick labels for better readability.\n",
        "plt.xticks(rotation=45)\n",
        "# Set the title for the heatmap, including key statistics about the transactions and thresholds.\n",
        "plt.title(f'Heating_Sub Rules Heatmap: {len(onehot_heating)} transactions, occurrence min-value of {n}, minimum support threshold of {minimum_support:.2f}')\n",
        "# Rotate the y-axis tick labels for better readability.\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Save the heatmap image to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/heatmaps/ahu/heating.png')\n",
        "# Display the heatmap on the screen.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3HQl-ElUbqB"
      },
      "outputs": [],
      "source": [
        "# Sort the rules based on support values in descending order.\n",
        "rules_heating.sort_values(by='support', ascending=False, inplace=True)\n",
        "\n",
        "# Create a new 'itemset' column by combining the 'antecedents' and 'consequents' into a single string.\n",
        "rules_heating['itemset'] = rules_heating.apply(lambda row: f\"{row['antecedents']},{row['consequents']}\", axis=1)\n",
        "\n",
        "# Calculate the count for each rule by multiplying its support by the total number of transactions.\n",
        "rules_heating['count'] = rules_heating.apply(lambda row: int(row['support'] * len(onehot_heating)), axis=1)\n",
        "\n",
        "# Convert the 'itemset' string representation back into a set for easier manipulation later.\n",
        "rules_heating['itemset'] = rules_heating['itemset'].apply(convert_str_to_set)\n",
        "\n",
        "# Round the support values to three decimal places for better readability in the output table.\n",
        "rules_heating['rounded_support'] = rules_heating.apply(lambda row: f\"{row['support']:.3f}\", axis=1)\n",
        "\n",
        "# Calculate the percentage representation of each rule's count relative to the total number of transactions.\n",
        "rules_heating['percentage'] = rules_heating.apply(lambda row: f\"{row['count'] * 100 / len(onehot_heating):.2f}%\", axis=1)\n",
        "\n",
        "# Remove any duplicate itemsets while keeping the first occurrence.\n",
        "rules_heating.drop_duplicates(subset=['itemset'], keep='first', inplace=True)\n",
        "\n",
        "# Reset the index of the DataFrame after dropping duplicates for cleaner output.\n",
        "rules_heating.reset_index(inplace=True)\n",
        "\n",
        "# Display the relevant columns: itemset, support, count, and percentage for verification.\n",
        "rules_heating[['itemset', 'support', 'count', 'percentage']]\n",
        "\n",
        "# Create a new figure for the table visualization with specified dimensions.\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "# Set the axes to 'tight' and turn off the axis display for a clean look.\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "# Generate a table to display the heating rules with rounded support, count, and percentage.\n",
        "ax.table(cellText=rules_heating[['itemset', 'rounded_support', 'count', 'percentage']].values,\n",
        "         colLabels=rules_heating[['itemset', 'support', 'count', 'percentage']].columns,\n",
        "         cellLoc='center', loc='center', fontsize=12)\n",
        "\n",
        "# Set the title for the table visualization.\n",
        "plt.title('Heating_Sub Rules')\n",
        "# Adjust layout to ensure everything fits nicely in the figure.\n",
        "plt.tight_layout()\n",
        "# Save the table image to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/tables/ahu/heating.png')\n",
        "# Display the table visualization on the screen.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33489ripUybB"
      },
      "source": [
        "## Cooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YooOv_4kpyqq"
      },
      "outputs": [],
      "source": [
        "# Extract service order descriptions for the Cooling subsystem and convert them into a list.\n",
        "corpus_cooling = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Cooling']['Description'].tolist()\n",
        "\n",
        "# Initialize the CountVectorizer to convert the text data into a matrix of token counts.\n",
        "vectorizer_cooling = CountVectorizer(ngram_range=(1, 1))\n",
        "\n",
        "# Fit the vectorizer on the cooling corpus and transform the text data into a document-term matrix (DTM).\n",
        "dtm_cooling = vectorizer_cooling.fit_transform(corpus_cooling)\n",
        "\n",
        "# Get the feature names (terms) used in the DTM for the cooling subsystem.\n",
        "terms_cooling = vectorizer_cooling.get_feature_names_out()\n",
        "\n",
        "# Convert the DTM from sparse matrix format to a dense DataFrame for easier manipulation and visualization.\n",
        "dtm_cooling = pd.DataFrame(dtm_cooling.toarray(), columns=terms_cooling)\n",
        "\n",
        "# Sum the term frequencies across all service orders to get the frequency of each term.\n",
        "term_frequencies_cooling = dtm_cooling.sum()\n",
        "\n",
        "# Generate a word cloud image based on the term frequencies, visualizing the most common terms.\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color='white').generate_from_frequencies(term_frequencies_cooling)\n",
        "\n",
        "# Display the generated word cloud using bilinear interpolation for smoother rendering.\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "\n",
        "# Turn off the axis display for a cleaner look at the word cloud.\n",
        "plt.axis('off')\n",
        "\n",
        "# Save the generated word cloud image to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/word_clouds/ahu/cooling.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHf1PFSvAiwC"
      },
      "outputs": [],
      "source": [
        "# Extract service order descriptions for the Cooling subsystem, splitting each description into individual words.\n",
        "transactions_cooling = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Cooling']['Description'].apply(lambda t: t.split(' '))\n",
        "\n",
        "# Convert the resulting series of lists into a list of transactions.\n",
        "transactions_cooling = list(transactions_cooling)\n",
        "\n",
        "# Initialize the TransactionEncoder to convert the list of transactions into a one-hot encoded format.\n",
        "encoder_cooling = TransactionEncoder().fit(transactions_cooling)\n",
        "\n",
        "# Transform the transactions into a one-hot encoded matrix format.\n",
        "onehot_cooling = encoder_cooling.transform(transactions_cooling)\n",
        "\n",
        "# Convert the one-hot encoded matrix into a DataFrame for easier manipulation and analysis.\n",
        "onehot_cooling = pd.DataFrame(onehot_cooling, columns=encoder_cooling.columns_)\n",
        "\n",
        "# Set the minimum number of occurrences required for an itemset to be considered frequent.\n",
        "'''\n",
        "Define the minimum number of occurrences for each itemset.\n",
        "This indirectly acts as a hyperparamether, so tune it as you pefer.\n",
        "Increase the value of \"n\" if you want to find more frequent itemsets.\n",
        "Decrease the value of \"n\" if you want to find less frequent itemsets.\n",
        "'''\n",
        "n = 2\n",
        "\n",
        "# Calculate the minimum support threshold based on the number of transactions.\n",
        "minimum_support = n / len(onehot_cooling)\n",
        "\n",
        "# Use the apriori algorithm to find frequent itemsets in the one-hot encoded data.\n",
        "frequent_itemsets_cooling = apriori(onehot_cooling, min_support=minimum_support, use_colnames=True, verbose=1)\n",
        "\n",
        "# Generate association rules from the frequent itemsets, using lift as the metric.\n",
        "rules_cooling = association_rules(frequent_itemsets_cooling, metric='lift', min_threshold=1)\n",
        "\n",
        "# Print summary statistics about the transactions and rules found.\n",
        "print(f'Number of transactions: {len(onehot_cooling)}')\n",
        "print(f'Minimum number of occurrences for each itemset: {n}')\n",
        "print(f'Minimum support threshold for itemsets: {minimum_support}')\n",
        "print(f'Number of frequent itemsets: {len(frequent_itemsets_cooling)}')\n",
        "print(f'Number of rules: {len(rules_cooling)}')\n",
        "\n",
        "# Replace frozen sets in the antecedents and consequents of the rules with comma-separated strings for better readability.\n",
        "rules_cooling['antecedents'] = rules_cooling['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_cooling['consequents'] = rules_cooling['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "# Transform the rules data into a matrix format suitable for generating a heatmap.\n",
        "pivot = rules_cooling.pivot(index='consequents', columns='antecedents', values='support')\n",
        "\n",
        "# Create a mask for the upper triangle of the heatmap to improve visual clarity.\n",
        "mask = np.triu(np.ones_like(pivot, dtype=bool))\n",
        "\n",
        "# Set the font scale for the seaborn heatmap.\n",
        "sns.set(font_scale=0.9)\n",
        "\n",
        "# Create a figure for the heatmap with specified dimensions.\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Generate the heatmap using the pivoted data, setting colors, annotations, and other visual parameters.\n",
        "heatmap = sns.heatmap(pivot, cmap='coolwarm', cbar=True, linewidths=0.5, linecolor='black', mask=mask, annot=True, fmt='.2f')\n",
        "\n",
        "# Rotate x-ticks for better readability.\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Set the title of the heatmap to reflect the analysis details.\n",
        "plt.title(f'Cooling_Sub Rules Heatmap: {len(onehot_cooling)} transactions, occurrence min-value of {n}, minimum support threshold of {minimum_support:.2f}')\n",
        "\n",
        "# Rotate y-ticks for better readability.\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Save the generated heatmap to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/heatmaps/ahu/cooling.png')\n",
        "\n",
        "# Display the heatmap.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlZfwYF4VhL1"
      },
      "outputs": [],
      "source": [
        "# Sort the rules by their support values in descending order for better visibility of the most significant rules.\n",
        "rules_cooling.sort_values(by='support', ascending=False, inplace=True)\n",
        "\n",
        "# Create a new column 'itemset' that combines the 'antecedents' and 'consequents' into a single string for easier interpretation.\n",
        "rules_cooling['itemset'] = rules_cooling.apply(lambda row: f\"{row['antecedents']},{row['consequents']}\", axis=1)\n",
        "\n",
        "# Calculate the count of occurrences for each rule by multiplying the support value with the total number of transactions.\n",
        "rules_cooling['count'] = rules_cooling.apply(lambda row: int(row['support'] * len(onehot_cooling)), axis=1)\n",
        "\n",
        "# Convert the 'itemset' strings back into sets for consistency and easier manipulation in later analysis.\n",
        "rules_cooling['itemset'] = rules_cooling['itemset'].apply(convert_str_to_set)\n",
        "\n",
        "# Round the support values to three decimal places for cleaner presentation in the output table.\n",
        "rules_cooling['rounded_support'] = rules_cooling.apply(lambda row: f\"{row['support']:.3f}\", axis=1)\n",
        "\n",
        "# Calculate the percentage of each rule's count relative to the total number of transactions for better interpretability.\n",
        "rules_cooling['percentage'] = rules_cooling.apply(lambda row: f\"{row['count'] * 100 / len(onehot_cooling):.2f}%\", axis=1)\n",
        "\n",
        "# Remove duplicate itemsets to ensure that each unique rule is only represented once in the output.\n",
        "rules_cooling.drop_duplicates(subset=['itemset'], keep='first', inplace=True)\n",
        "\n",
        "# Reset the index of the DataFrame for cleaner presentation and easier access to rows.\n",
        "rules_cooling.reset_index(inplace=True)\n",
        "\n",
        "# Create a new figure for displaying the rules in a table format.\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Set the axis to be tight and turn off the axis visibility to focus on the table itself.\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "# Create a table to display the relevant columns of the rules DataFrame, including the formatted values for clarity.\n",
        "ax.table(cellText=rules_cooling[['itemset', 'rounded_support', 'count', 'percentage']].values,\n",
        "         colLabels=rules_cooling[['itemset', 'support', 'count', 'percentage']].columns,\n",
        "         cellLoc='center', loc='center', fontsize=12)\n",
        "\n",
        "# Set the title for the table to describe what the rules represent.\n",
        "plt.title('Cooling_Sub Rules')\n",
        "\n",
        "# Adjust the layout to ensure everything fits well without overlap.\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the generated table as a PNG file to the specified path for future reference or inclusion in reports.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/tables/ahu/cooling.png')\n",
        "\n",
        "# Display the table visually.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3ilv0NlWeIC"
      },
      "source": [
        "## Casing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_A8de45qnVv"
      },
      "outputs": [],
      "source": [
        "corpus_casing = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Casing']['Description'].tolist()\n",
        "vectorizer_casing = CountVectorizer(ngram_range=(1, 1))\n",
        "dtm_casing = vectorizer_casing.fit_transform(corpus_casing)\n",
        "terms_casing = vectorizer_casing.get_feature_names_out()\n",
        "dtm_casing = pd.DataFrame(dtm_casing.toarray(), columns=terms_casing)\n",
        "term_frequencies_casing = dtm_casing.sum()\n",
        "\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color='white').generate_from_frequencies(term_frequencies_casing)\n",
        "\n",
        "# plt.title('Casing Word Cloud')\n",
        "# plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/word_clouds/ahu/casing.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkuD7KFtEm31"
      },
      "outputs": [],
      "source": [
        "transactions_casing = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Casing']['Description'].apply(lambda t: t.split(' '))\n",
        "transactions_casing = list(transactions_casing)\n",
        "encoder_casing = TransactionEncoder().fit(transactions_casing)\n",
        "onehot_casing = encoder_casing.transform(transactions_casing)\n",
        "onehot_casing = pd.DataFrame(onehot_casing, columns=encoder_casing.columns_)\n",
        "\n",
        "n = 2\n",
        "minimum_support = n/len(onehot_casing)\n",
        "frequent_itemsets_casing = apriori(onehot_casing,\n",
        "                            min_support =  minimum_support,\n",
        "                            use_colnames = True,\n",
        "                            verbose = 1)\n",
        "\n",
        "rules_casing = association_rules(frequent_itemsets_casing, metric = 'lift', min_threshold = 1)\n",
        "\n",
        "print(f'Number of transactions: {len(onehot_casing)}')\n",
        "print(f'Minimum number of occurrence for each itemset: {n}')\n",
        "print(f'Minimum support threshold for itemsets: {minimum_support}')\n",
        "print(f'number of frequent itemsets: {len(frequent_itemsets_casing)}')\n",
        "print(f'Number of rules: {len(rules_casing)}')\n",
        "\n",
        "# Replace frozen sets with strings.\n",
        "rules_casing['antecedents'] = rules_casing['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_casing['consequents'] = rules_casing['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "# Transform data to matrix format and generate heatmap.\n",
        "pivot = rules_casing.pivot(index='consequents', columns='antecedents', values='support')\n",
        "\n",
        "mask = np.triu(np.ones_like(pivot, dtype=bool))\n",
        "\n",
        "sns.set(font_scale=0.9)\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "heatmap = sns.heatmap(pivot, cmap='coolwarm', cbar=True, linewidths=0.5, linecolor='black', mask=mask, annot=True, fmt='.2f')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(f'Casing Rules Heatmap: {len(onehot_casing)} transactions, occurrence min-value of {n}, minimum support threshold of {minimum_support:.2f}')\n",
        "plt.yticks(rotation=0)\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/heatmaps/ahu/casing.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8siUUkUXPnQ"
      },
      "outputs": [],
      "source": [
        "rules_casing.sort_values(by='support', ascending=False, inplace=True)\n",
        "rules_casing['itemset'] = rules_casing.apply(lambda row: f\"{row['antecedents']},{row['consequents']}\", axis=1)\n",
        "rules_casing['count'] = rules_casing.apply(lambda row: int(row['support']*len(onehot_casing)), axis=1)\n",
        "rules_casing['itemset'] = rules_casing['itemset'].apply(convert_str_to_set)\n",
        "rules_casing['rounded_support'] = rules_casing.apply(lambda row: f\"{row['support']:.3f}\", axis=1)\n",
        "rules_casing['percentage'] = rules_casing.apply(lambda row: f\"{row['count']*100/len(onehot_casing):.2f}%\", axis=1)\n",
        "rules_casing.drop_duplicates(subset=['itemset'], keep='first', inplace=True)\n",
        "rules_casing.reset_index(inplace=True)\n",
        "rules_casing[['itemset', 'support', 'count', 'percentage']]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "ax.table(cellText=rules_casing[['itemset', 'rounded_support', 'count', 'percentage']].values,\n",
        "         colLabels=rules_casing[['itemset', 'support', 'count', 'percentage']].columns,\n",
        "         cellLoc='center', loc='center', fontsize=12)\n",
        "\n",
        "plt.title('Casing Rules')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/tables/ahu/casing.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYhgzgjMXlaE"
      },
      "source": [
        "## Humidifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4QNpOz7rFsQ"
      },
      "outputs": [],
      "source": [
        "corpus_humidifier = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Humidifier']['Description'].tolist()\n",
        "vectorizer_humidifier = CountVectorizer(ngram_range=(1, 1))\n",
        "dtm_humidifier = vectorizer_humidifier.fit_transform(corpus_humidifier)\n",
        "terms_humidifier = vectorizer_humidifier.get_feature_names_out()\n",
        "dtm_humidifier = pd.DataFrame(dtm_humidifier.toarray(), columns=terms_humidifier)\n",
        "term_frequencies_humidifier = dtm_humidifier.sum()\n",
        "\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color='white').generate_from_frequencies(term_frequencies_humidifier)\n",
        "\n",
        "# plt.title('Humidifier Word Cloud')\n",
        "# plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/word_clouds/ahu/humidifier.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y45sRhUZFzfr"
      },
      "outputs": [],
      "source": [
        "transactions_humidifier = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Humidifier']['Description'].apply(lambda t: t.split(' '))\n",
        "transactions_humidifier = list(transactions_humidifier)\n",
        "encoder_humidifier = TransactionEncoder().fit(transactions_humidifier)\n",
        "onehot_humidifier = encoder_humidifier.transform(transactions_humidifier)\n",
        "onehot_humidifier = pd.DataFrame(onehot_humidifier, columns=encoder_humidifier.columns_)\n",
        "\n",
        "n = 5\n",
        "minimum_support = n/len(onehot_humidifier)\n",
        "frequent_itemsets_humidifier = apriori(onehot_humidifier,\n",
        "                            min_support =  minimum_support,\n",
        "                            use_colnames = True,\n",
        "                            verbose = 1)\n",
        "\n",
        "rules_humidifier = association_rules(frequent_itemsets_humidifier, metric = 'lift', min_threshold = 1)\n",
        "\n",
        "print(f'Number of transactions: {len(onehot_humidifier)}')\n",
        "print(f'Minimum number of occurrence for each itemset: {n}')\n",
        "print(f'Minimum support threshold for itemsets: {minimum_support}')\n",
        "print(f'number of frequent itemsets: {len(frequent_itemsets_humidifier)}')\n",
        "print(f'Number of rules: {len(rules_humidifier)}')\n",
        "\n",
        "# Replace frozen sets with strings.\n",
        "rules_humidifier['antecedents'] = rules_humidifier['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_humidifier['consequents'] = rules_humidifier['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "# Transform data to matrix format and generate heatmap.\n",
        "pivot = rules_humidifier.pivot(index='consequents', columns='antecedents', values='support')\n",
        "\n",
        "mask = np.triu(np.ones_like(pivot, dtype=bool))\n",
        "\n",
        "sns.set(font_scale=0.9)\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "heatmap = sns.heatmap(pivot, cmap='coolwarm', cbar=True, linewidths=0.5, linecolor='black', mask=mask, annot=True, fmt='.2f')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(f'Humidifier Rules Heatmap: {len(onehot_humidifier)} transactions, occurrence min-value of {n}, minimum support threshold of {minimum_support:.2f}')\n",
        "plt.yticks(rotation=0)\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/heatmaps/ahu/humidifier.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oucHTneKYGqm"
      },
      "outputs": [],
      "source": [
        "rules_humidifier.sort_values(by='support', ascending=False, inplace=True)\n",
        "rules_humidifier['itemset'] = rules_humidifier.apply(lambda row: f\"{row['antecedents']},{row['consequents']}\", axis=1)\n",
        "rules_humidifier['count'] = rules_humidifier.apply(lambda row: int(row['support']*len(onehot_humidifier)), axis=1)\n",
        "rules_humidifier['itemset'] = rules_humidifier['itemset'].apply(convert_str_to_set)\n",
        "rules_humidifier['rounded_support'] = rules_humidifier.apply(lambda row: f\"{row['support']:.3f}\", axis=1)\n",
        "rules_humidifier['percentage'] = rules_humidifier.apply(lambda row: f\"{row['count']*100/len(onehot_humidifier):.2f}%\", axis=1)\n",
        "rules_humidifier.drop_duplicates(subset=['itemset'], keep='first', inplace=True)\n",
        "rules_humidifier.reset_index(inplace=True)\n",
        "rules_humidifier[['itemset', 'support', 'count', 'percentage']]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "ax.table(cellText=rules_humidifier[['itemset', 'rounded_support', 'count', 'percentage']].values,\n",
        "         colLabels=rules_humidifier[['itemset', 'support', 'count', 'percentage']].columns,\n",
        "         cellLoc='center', loc='center', fontsize=12)\n",
        "\n",
        "plt.title('Humidifier Rules')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/tables/ahu/humidifier.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUC3I-C_Ye3_"
      },
      "source": [
        "## Heat Exchanger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s1GtXBirbfr"
      },
      "outputs": [],
      "source": [
        "corpus_heatExchanger = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Heat Exchanger']['Description'].tolist()\n",
        "vectorizer_heatExchanger = CountVectorizer(ngram_range=(1, 1))\n",
        "dtm_heatExchanger = vectorizer_heatExchanger.fit_transform(corpus_heatExchanger)\n",
        "terms_heatExchanger = vectorizer_heatExchanger.get_feature_names_out()\n",
        "dtm_heatExchanger = pd.DataFrame(dtm_heatExchanger.toarray(), columns=terms_heatExchanger)\n",
        "term_frequencies_heatExchanger = dtm_heatExchanger.sum()\n",
        "\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color='white').generate_from_frequencies(term_frequencies_heatExchanger)\n",
        "\n",
        "# plt.title('Heat Exchanger Word Cloud')\n",
        "# plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/word_clouds/ahu/heat_exchanger.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6C5ion5G5TV"
      },
      "outputs": [],
      "source": [
        "transactions_heatExchanger = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Heat Exchanger']['Description'].apply(lambda t: t.split(' '))\n",
        "transactions_heatExchanger = list(transactions_heatExchanger)\n",
        "encoder_heatExchanger = TransactionEncoder().fit(transactions_heatExchanger)\n",
        "onehot_heatExchanger = encoder_heatExchanger.transform(transactions_heatExchanger)\n",
        "onehot_heatExchanger = pd.DataFrame(onehot_heatExchanger, columns=encoder_heatExchanger.columns_)\n",
        "\n",
        "n = 2\n",
        "minimum_support = n/len(onehot_heatExchanger)\n",
        "frequent_itemsets_heatExchanger = apriori(onehot_heatExchanger,\n",
        "                            min_support =  minimum_support,\n",
        "                            use_colnames = True,\n",
        "                            verbose = 1)\n",
        "\n",
        "rules_heatExchanger = association_rules(frequent_itemsets_heatExchanger, metric = 'lift', min_threshold = 1)\n",
        "\n",
        "print(f'Number of transactions: {len(onehot_heatExchanger)}')\n",
        "print(f'Minimum number of occurrence for each itemset: {n}')\n",
        "print(f'Minimum support threshold for itemsets: {minimum_support}')\n",
        "print(f'number of frequent itemsets: {len(frequent_itemsets_heatExchanger)}')\n",
        "print(f'Number of rules: {len(rules_heatExchanger)}')\n",
        "\n",
        "# Replace frozen sets with strings.\n",
        "rules_heatExchanger['antecedents'] = rules_heatExchanger['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_heatExchanger['consequents'] = rules_heatExchanger['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "# Transform data to matrix format and generate heatmap.\n",
        "pivot = rules_heatExchanger.pivot(index='consequents', columns='antecedents', values='support')\n",
        "\n",
        "mask = np.triu(np.ones_like(pivot, dtype=bool))\n",
        "\n",
        "sns.set(font_scale=0.9)\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "heatmap = sns.heatmap(pivot, cmap='coolwarm', cbar=True, linewidths=0.5, linecolor='black', mask=mask, annot=True, fmt='.2f')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(f'Heat Exchanger Rules Heatmap: {len(onehot_heatExchanger)} transactions, occurrence min-value of {n}, minimum support threshold of {minimum_support:.2f}')\n",
        "plt.yticks(rotation=0)\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/heatmaps/ahu/heat_exchanger.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uadsG1nAYjVi"
      },
      "outputs": [],
      "source": [
        "rules_heatExchanger.sort_values(by='support', ascending=False, inplace=True)\n",
        "rules_heatExchanger['itemset'] = rules_heatExchanger.apply(lambda row: f\"{row['antecedents']},{row['consequents']}\", axis=1)\n",
        "rules_heatExchanger['count'] = rules_heatExchanger.apply(lambda row: int(row['support']*len(onehot_heatExchanger)), axis=1)\n",
        "rules_heatExchanger['itemset'] = rules_heatExchanger['itemset'].apply(convert_str_to_set)\n",
        "rules_heatExchanger['rounded_support'] = rules_heatExchanger.apply(lambda row: f\"{row['support']:.3f}\", axis=1)\n",
        "rules_heatExchanger['percentage'] = rules_heatExchanger.apply(lambda row: f\"{row['count']*100/len(onehot_heatExchanger):.2f}%\", axis=1)\n",
        "rules_heatExchanger.drop_duplicates(subset=['itemset'], keep='first', inplace=True)\n",
        "rules_heatExchanger.reset_index(inplace=True)\n",
        "rules_heatExchanger[['itemset', 'support', 'count', 'percentage']]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "ax.table(cellText=rules_heatExchanger[['itemset', 'rounded_support', 'count', 'percentage']].values,\n",
        "         colLabels=rules_heatExchanger[['itemset', 'support', 'count', 'percentage']].columns,\n",
        "         cellLoc='center', loc='center', fontsize=12)\n",
        "\n",
        "plt.title('Heat Exchanger Rules')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/tables/ahu/heat_exchanger.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pC6T8teZKS3"
      },
      "source": [
        "## Damper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T88wlixLr0Rn"
      },
      "outputs": [],
      "source": [
        "corpus_damper = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Damper']['Description'].tolist()\n",
        "vectorizer_damper = CountVectorizer(ngram_range=(1, 1))\n",
        "dtm_damper = vectorizer_damper.fit_transform(corpus_damper)\n",
        "terms_damper = vectorizer_damper.get_feature_names_out()\n",
        "dtm_damper = pd.DataFrame(dtm_damper.toarray(), columns=terms_damper)\n",
        "term_frequencies_damper = dtm_damper.sum()\n",
        "\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color='white').generate_from_frequencies(term_frequencies_damper)\n",
        "\n",
        "# plt.title('Damper Word Cloud')\n",
        "# plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/word_clouds/ahu/damper.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpdzmpdpHn-g"
      },
      "outputs": [],
      "source": [
        "transactions_damper = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Damper']['Description'].apply(lambda t: t.split(' '))\n",
        "transactions_damper = list(transactions_damper)\n",
        "encoder_damper = TransactionEncoder().fit(transactions_damper)\n",
        "onehot_damper = encoder_damper.transform(transactions_damper)\n",
        "onehot_damper = pd.DataFrame(onehot_damper, columns=encoder_damper.columns_)\n",
        "\n",
        "n = 1\n",
        "minimum_support = n/len(onehot_damper)\n",
        "frequent_itemsets_damper = apriori(onehot_damper,\n",
        "                            min_support =  minimum_support,\n",
        "                            use_colnames = True,\n",
        "                            verbose = 1)\n",
        "\n",
        "rules_damper = association_rules(frequent_itemsets_damper, metric = 'lift', min_threshold = 1)\n",
        "\n",
        "print(f'Number of transactions: {len(onehot_damper)}')\n",
        "print(f'Minimum number of occurrence for each itemset: {n}')\n",
        "print(f'Minimum support threshold for itemsets: {minimum_support}')\n",
        "print(f'number of frequent itemsets: {len(frequent_itemsets_damper)}')\n",
        "print(f'Number of rules: {len(rules_damper)}')\n",
        "\n",
        "# Replace frozen sets with strings.\n",
        "rules_damper['antecedents'] = rules_damper['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_damper['consequents'] = rules_damper['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "# Transform data to matrix format and generate heatmap.\n",
        "pivot = rules_damper.pivot(index='consequents', columns='antecedents', values='support')\n",
        "\n",
        "mask = np.triu(np.ones_like(pivot, dtype=bool))\n",
        "\n",
        "sns.set(font_scale=0.9)\n",
        "plt.figure(figsize=(15, 8))\n",
        "heatmap = sns.heatmap(pivot, cmap='coolwarm', cbar=True, linewidths=0.5, linecolor='black', mask=mask, annot=True, fmt='.2f')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(f'Damper Rules Heatmap: {len(onehot_damper)} transactions, occurrence min-value of {n}, minimum support threshold of {minimum_support:.2f}')\n",
        "plt.yticks(rotation=0)\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/heatmaps/ahu/damper.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4775cpxZN9M"
      },
      "outputs": [],
      "source": [
        "rules_damper.sort_values(by='support', ascending=False, inplace=True)\n",
        "rules_damper['itemset'] = rules_damper.apply(lambda row: f\"{row['antecedents']},{row['consequents']}\", axis=1)\n",
        "rules_damper['count'] = rules_damper.apply(lambda row: int(row['support']*len(onehot_damper)), axis=1)\n",
        "rules_damper['itemset'] = rules_damper['itemset'].apply(convert_str_to_set)\n",
        "rules_damper['rounded_support'] = rules_damper.apply(lambda row: f\"{row['support']:.3f}\", axis=1)\n",
        "rules_damper['percentage'] = rules_damper.apply(lambda row: f\"{row['count']*100/len(onehot_damper):.2f}%\", axis=1)\n",
        "rules_damper.drop_duplicates(subset=['itemset'], keep='first', inplace=True)\n",
        "rules_damper.reset_index(inplace=True)\n",
        "rules_damper[['itemset', 'support', 'count', 'percentage']]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "ax.table(cellText=rules_damper[['itemset', 'rounded_support', 'count', 'percentage']].values,\n",
        "         colLabels=rules_damper[['itemset', 'support', 'count', 'percentage']].columns,\n",
        "         cellLoc='center', loc='center', fontsize=12)\n",
        "\n",
        "plt.title('Damper Rules')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/tables/ahu/damper.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyCoxmkXWohl"
      },
      "source": [
        "## Undefined Malfunctions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQq6cH0zqLQ5"
      },
      "outputs": [],
      "source": [
        "# Extract the service order descriptions related to 'Undefined Malfunctions' submodule into a list.\n",
        "corpus_undifiedMalfunctions = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Undefined Malfunctions']['Description'].tolist()\n",
        "\n",
        "# Initialize the CountVectorizer to convert the text data into a document-term matrix (DTM) with unigrams (single words).\n",
        "vectorizer_undifiedMalfunctions = CountVectorizer(ngram_range=(1, 1))\n",
        "\n",
        "# Fit the vectorizer to the corpus and transform the text data into a document-term matrix.\n",
        "dtm_undifiedMalfunctions = vectorizer_undifiedMalfunctions.fit_transform(corpus_undifiedMalfunctions)\n",
        "\n",
        "# Retrieve the feature names (terms) from the vectorizer after fitting it to the corpus.\n",
        "terms_undifiedMalfunctions = vectorizer_undifiedMalfunctions.get_feature_names_out()\n",
        "\n",
        "# Convert the sparse DTM to a dense DataFrame for easier manipulation and analysis.\n",
        "dtm_undifiedMalfunctions = pd.DataFrame(dtm_undifiedMalfunctions.toarray(), columns=terms_undifiedMalfunctions)\n",
        "\n",
        "# Calculate the term frequencies by summing the counts of each term across all documents in the DTM.\n",
        "term_frequencies_undifiedMalfunctions = dtm_undifiedMalfunctions.sum()\n",
        "\n",
        "# Generate a word cloud image based on the term frequencies calculated earlier, visualizing the most common words.\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color='white').generate_from_frequencies(term_frequencies_undifiedMalfunctions)\n",
        "\n",
        "# Display the generated word cloud image.\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "\n",
        "# Turn off the axis for a cleaner presentation of the word cloud.\n",
        "plt.axis('off')\n",
        "\n",
        "# Save the word cloud image to a specified file path for future reference or inclusion in reports.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/word_clouds/ahu/undefined_malfunctions.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVfFOSeXDvph"
      },
      "outputs": [],
      "source": [
        "# Extract the service order descriptions related to 'Undefined Malfunctions' submodule into a list, splitting each description into words.\n",
        "transactions_undifiedMalfunctions = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Undefined Malfunctions']['Description'].apply(lambda t: t.split(' '))\n",
        "# Convert the Series of lists into a list of transactions.\n",
        "transactions_undifiedMalfunctions = list(transactions_undifiedMalfunctions)\n",
        "\n",
        "# Initialize the TransactionEncoder to convert the list of transactions into a one-hot encoded format.\n",
        "encoder_undifiedMalfunctions = TransactionEncoder().fit(transactions_undifiedMalfunctions)\n",
        "# Transform the transactions into one-hot encoded format.\n",
        "onehot_undifiedMalfunctions = encoder_undifiedMalfunctions.transform(transactions_undifiedMalfunctions)\n",
        "# Convert the one-hot encoded data into a DataFrame for easier analysis.\n",
        "onehot_undifiedMalfunctions = pd.DataFrame(onehot_undifiedMalfunctions, columns=encoder_undifiedMalfunctions.columns_)\n",
        "\n",
        "# Define the minimum number of occurrences for an itemset to be considered frequent.\n",
        "n = 10\n",
        "# Calculate the minimum support threshold based on the total number of transactions.\n",
        "minimum_support = n / len(onehot_undifiedMalfunctions)\n",
        "\n",
        "# Apply the Apriori algorithm to find frequent itemsets in the one-hot encoded data.\n",
        "frequent_itemsets_undifiedMalfunctions = apriori(onehot_undifiedMalfunctions,\n",
        "                            min_support=minimum_support,\n",
        "                            use_colnames=True,\n",
        "                            verbose=1)\n",
        "\n",
        "# Generate association rules from the frequent itemsets using the lift metric.\n",
        "rules_undifiedMalfunctions = association_rules(frequent_itemsets_undifiedMalfunctions, metric='lift', min_threshold=1)\n",
        "\n",
        "# Print summary statistics about the transactions and rules generated.\n",
        "print(f'Number of transactions: {len(onehot_undifiedMalfunctions)}')\n",
        "print(f'Minimum number of occurrence for each itemset: {n}')\n",
        "print(f'Minimum support threshold for itemsets: {minimum_support}')\n",
        "print(f'Number of frequent itemsets: {len(frequent_itemsets_undifiedMalfunctions)}')\n",
        "print(f'Number of rules: {len(rules_undifiedMalfunctions)}')\n",
        "\n",
        "# Replace frozen sets in the antecedents and consequents with strings for better readability.\n",
        "rules_undifiedMalfunctions['antecedents'] = rules_undifiedMalfunctions['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_undifiedMalfunctions['consequents'] = rules_undifiedMalfunctions['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "# Transform data to matrix format for generating a heatmap of the rules.\n",
        "pivot = rules_undifiedMalfunctions.pivot(index='consequents', columns='antecedents', values='support')\n",
        "\n",
        "# Create a mask to hide the upper triangle of the heatmap.\n",
        "mask = np.triu(np.ones_like(pivot, dtype=bool))\n",
        "\n",
        "# Set the font scale for the seaborn plot.\n",
        "sns.set(font_scale=0.9)\n",
        "# Create a figure for the heatmap with specified dimensions.\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Generate the heatmap using the pivot table data, specifying color map and annotation options.\n",
        "heatmap = sns.heatmap(pivot, cmap='coolwarm', cbar=True, linewidths=0.5, linecolor='black', mask=mask, annot=True, fmt='.2f')\n",
        "\n",
        "# Rotate x-ticks for better visibility.\n",
        "plt.xticks(rotation=45)\n",
        "# Set the title for the heatmap with transaction and support information.\n",
        "plt.title(f'Undefined Malfunctions Rules Heatmap: {len(onehot_cooling)} transactions, occurrence min-value of {n}, minimum support threshold of {minimum_support:.2f}')\n",
        "# Rotate y-ticks for better visibility.\n",
        "plt.yticks(rotation=0)\n",
        "# Save the heatmap figure to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/heatmaps/ahu/undefined_malfunctions.png')\n",
        "# Display the heatmap.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI70a4p5VbMx"
      },
      "outputs": [],
      "source": [
        "# Sort the rules by support in descending order for better readability.\n",
        "rules_undifiedMalfunctions.sort_values(by='support', ascending=False, inplace=True)\n",
        "\n",
        "# Create a new column 'itemset' that combines the antecedents and consequents into a single string.\n",
        "rules_undifiedMalfunctions['itemset'] = rules_undifiedMalfunctions.apply(lambda row: f\"{row['antecedents']},{row['consequents']}\", axis=1)\n",
        "\n",
        "# Calculate the count of each itemset based on its support and the total number of transactions.\n",
        "rules_undifiedMalfunctions['count'] = rules_undifiedMalfunctions.apply(lambda row: int(row['support'] * len(onehot_undifiedMalfunctions)), axis=1)\n",
        "\n",
        "# Convert the 'itemset' strings into sets for easier manipulation in future analyses.\n",
        "rules_undifiedMalfunctions['itemset'] = rules_undifiedMalfunctions['itemset'].apply(convert_str_to_set)\n",
        "\n",
        "# Round the support values to three decimal places for easier readability.\n",
        "rules_undifiedMalfunctions['rounded_support'] = rules_undifiedMalfunctions.apply(lambda row: f\"{row['support']:.3f}\", axis=1)\n",
        "\n",
        "# Calculate the percentage of each itemset's occurrences relative to the total transactions.\n",
        "rules_undifiedMalfunctions['percentage'] = rules_undifiedMalfunctions.apply(lambda row: f\"{row['count'] * 100 / len(onehot_undifiedMalfunctions):.2f}%\", axis=1)\n",
        "\n",
        "# Remove duplicate itemsets, keeping only the first occurrence.\n",
        "rules_undifiedMalfunctions.drop_duplicates(subset=['itemset'], keep='first', inplace=True)\n",
        "\n",
        "# Reset the index of the DataFrame for cleaner output.\n",
        "rules_undifiedMalfunctions.reset_index(inplace=True)\n",
        "\n",
        "# Select relevant columns to display.\n",
        "rules_undifiedMalfunctions[['itemset', 'support', 'count', 'percentage']]\n",
        "\n",
        "# Create a figure and axis for the table visualization.\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Set the axis to tight layout and turn off the axis display for clarity.\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "# Create a table to display the itemsets along with their rounded support, count, and percentage.\n",
        "ax.table(cellText=rules_undifiedMalfunctions[['itemset', 'rounded_support', 'count', 'percentage']].values,\n",
        "         colLabels=rules_undifiedMalfunctions[['itemset', 'support', 'count', 'percentage']].columns,\n",
        "         cellLoc='center', loc='center', fontsize=12)\n",
        "\n",
        "# Set the title for the table.\n",
        "plt.title('Undefined Malfunctions Rules')\n",
        "\n",
        "# Adjust the layout to prevent clipping of table elements.\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/tables/ahu/undefined_malfunctions.png')\n",
        "\n",
        "# Display the table.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAxQGcb0Zq8l"
      },
      "source": [
        "## Unknown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeSDzjlhsKDy"
      },
      "outputs": [],
      "source": [
        "# Extract the descriptions of service orders that are categorized as 'Uncategorized'.\n",
        "corpus_unknown = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Uncategorized']['Description'].tolist()\n",
        "\n",
        "# Initialize the CountVectorizer to convert the text data into a matrix of token counts, considering unigrams.\n",
        "vectorizer_unknown = CountVectorizer(ngram_range=(1, 1))\n",
        "\n",
        "# Fit the vectorizer to the corpus and transform it into a document-term matrix (DTM).\n",
        "dtm_unknown = vectorizer_unknown.fit_transform(corpus_unknown)\n",
        "\n",
        "# Get the feature names (unique terms) from the CountVectorizer.\n",
        "terms_unknown = vectorizer_unknown.get_feature_names_out()\n",
        "\n",
        "# Convert the DTM to a DataFrame for easier manipulation and analysis, using the extracted terms as column headers.\n",
        "dtm_unknown = pd.DataFrame(dtm_unknown.toarray(), columns=terms_unknown)\n",
        "\n",
        "# Calculate the term frequencies by summing the occurrences of each term across all documents.\n",
        "term_frequencies_unknown = dtm_unknown.sum()\n",
        "\n",
        "# Generate a word cloud image from the term frequencies, specifying the size and background color.\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color='white').generate_from_frequencies(term_frequencies_unknown)\n",
        "\n",
        "# Display the generated word cloud.\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "\n",
        "# Turn off the axis display for a cleaner visualization.\n",
        "plt.axis('off')\n",
        "\n",
        "# Save the word cloud image to the specified file path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/word_clouds/ahu/unknown.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2qClAjiIlPN"
      },
      "outputs": [],
      "source": [
        "# Extract the descriptions of service orders that are categorized as 'Uncategorized'.\n",
        "transactions_unknown = df_so_cleaned_ventilation[df_so_cleaned_ventilation['submodule'] == 'Uncategorized']['Description'].apply(lambda t: t.split(' '))\n",
        "\n",
        "# Convert the Series of lists into a list of transactions for further analysis.\n",
        "transactions_unknown = list(transactions_unknown)\n",
        "\n",
        "# Initialize the TransactionEncoder to convert the list of transactions into a one-hot encoded format.\n",
        "encoder_unknown = TransactionEncoder().fit(transactions_unknown)\n",
        "\n",
        "# Transform the transactions into a one-hot encoded DataFrame.\n",
        "onehot_unknown = encoder_unknown.transform(transactions_unknown)\n",
        "onehot_unknown = pd.DataFrame(onehot_unknown, columns=encoder_unknown.columns_)\n",
        "\n",
        "# Set the minimum occurrence threshold for itemsets.\n",
        "'''\n",
        "Define the minimum number of occurrences for each itemset.\n",
        "This indirectly acts as a hyperparamether, so tune it as you pefer.\n",
        "Increase the value of \"n\" if you want to find more frequent itemsets.\n",
        "Decrease the value of \"n\" if you want to find less frequent itemsets.\n",
        "'''\n",
        "n = 2\n",
        "\n",
        "# Calculate the minimum support threshold based on the total number of transactions.\n",
        "minimum_support = n / len(onehot_unknown)\n",
        "\n",
        "# Generate frequent itemsets using the Apriori algorithm with the specified minimum support.\n",
        "frequent_itemsets_unknown = apriori(onehot_unknown, min_support=minimum_support, use_colnames=True, verbose=1)\n",
        "\n",
        "# Generate association rules from the frequent itemsets using 'lift' as the metric.\n",
        "rules_unknown = association_rules(frequent_itemsets_unknown, metric='lift', min_threshold=1)\n",
        "\n",
        "# Print out relevant information regarding the transactions and rules generated.\n",
        "print(f'Number of transactions: {len(onehot_unknown)}')\n",
        "print(f'Minimum number of occurrence for each itemset: {n}')\n",
        "print(f'Minimum support threshold for itemsets: {minimum_support}')\n",
        "print(f'Number of frequent itemsets: {len(frequent_itemsets_unknown)}')\n",
        "print(f'Number of rules: {len(rules_unknown)}')\n",
        "\n",
        "# Replace frozen sets with strings for easier readability.\n",
        "rules_unknown['antecedents'] = rules_unknown['antecedents'].apply(lambda a: ','.join(list(a)))\n",
        "rules_unknown['consequents'] = rules_unknown['consequents'].apply(lambda a: ','.join(list(a)))\n",
        "\n",
        "# Transform data to matrix format for visualization and generate a heatmap of the rules.\n",
        "pivot = rules_unknown.pivot(index='consequents', columns='antecedents', values='support')\n",
        "\n",
        "# Create a mask to hide the upper triangle of the heatmap.\n",
        "mask = np.triu(np.ones_like(pivot, dtype=bool))\n",
        "\n",
        "# Set the font scale for the heatmap annotations.\n",
        "sns.set(font_scale=0.9)\n",
        "\n",
        "# Create a figure for the heatmap with specified dimensions.\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Generate the heatmap using the pivoted DataFrame, with annotations for support values.\n",
        "heatmap = sns.heatmap(pivot, cmap='coolwarm', cbar=True, linewidths=0.5, linecolor='black', mask=mask, annot=True, fmt='.2f')\n",
        "\n",
        "# Rotate the x-axis labels for better visibility.\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Set the title of the heatmap, including the number of transactions and thresholds.\n",
        "plt.title(f'Unknown Rules Heatmap: {len(onehot_unknown)} transactions, occurrence min-value of {n}, minimum support threshold of {minimum_support:.2f}')\n",
        "\n",
        "# Rotate the y-axis labels for better visibility.\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Save the heatmap as an image file to the specified path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/heatmaps/ahu/unknown.png')\n",
        "\n",
        "# Display the heatmap.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9KCG9QdZytW"
      },
      "outputs": [],
      "source": [
        "# Sort the rules by support in descending order.\n",
        "rules_unknown.sort_values(by='support', ascending=False, inplace=True)\n",
        "\n",
        "# Create a new column 'itemset' that combines the antecedents and consequents into a single string.\n",
        "rules_unknown['itemset'] = rules_unknown.apply(lambda row: f\"{row['antecedents']},{row['consequents']}\", axis=1)\n",
        "\n",
        "# Calculate the count of occurrences for each rule based on its support and the total number of transactions.\n",
        "rules_unknown['count'] = rules_unknown.apply(lambda row: int(row['support'] * len(onehot_unknown)), axis=1)\n",
        "\n",
        "# Convert the 'itemset' strings to sets for easier manipulation and comparison.\n",
        "rules_unknown['itemset'] = rules_unknown['itemset'].apply(convert_str_to_set)\n",
        "\n",
        "# Round the support values to three decimal places for better readability.\n",
        "rules_unknown['rounded_support'] = rules_unknown.apply(lambda row: f\"{row['support']:.3f}\", axis=1)\n",
        "\n",
        "# Calculate the percentage of occurrences for each itemset based on the total number of transactions.\n",
        "rules_unknown['percentage'] = rules_unknown.apply(lambda row: f\"{row['count'] * 100 / len(onehot_unknown):.2f}%\", axis=1)\n",
        "\n",
        "# Remove duplicate itemsets, keeping only the first occurrence.\n",
        "rules_unknown.drop_duplicates(subset=['itemset'], keep='first', inplace=True)\n",
        "\n",
        "# Reset the index of the DataFrame to ensure proper indexing after sorting and removing duplicates.\n",
        "rules_unknown.reset_index(inplace=True)\n",
        "\n",
        "# Display selected columns of the rules DataFrame for verification.\n",
        "rules_unknown[['itemset', 'support', 'count', 'percentage']]\n",
        "\n",
        "# Create a figure and axis for the table visualization.\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "\n",
        "# Set the axis properties to 'tight' to optimize layout and turn off axis visibility.\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "# Create a table using the specified cell text and column labels.\n",
        "ax.table(cellText=rules_unknown[['itemset', 'rounded_support', 'count', 'percentage']].values,\n",
        "         colLabels=rules_unknown[['itemset', 'support', 'count', 'percentage']].columns,\n",
        "         cellLoc='center', loc='center', fontsize=12)\n",
        "\n",
        "# Set the title for the table.\n",
        "plt.title('Unknown Rules')\n",
        "\n",
        "# Adjust the layout to fit the table nicely within the figure.\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the table as an image file to the specified path.\n",
        "plt.savefig('/content/drive/MyDrive/Kropman/figures/tables/ahu/unknown.png')\n",
        "\n",
        "# Display the table visualization.\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMdUOEUy+ZxcKuXWfFvuhkn",
      "collapsed_sections": [
        "7595UIXVFovr",
        "xPUgLKbTr4tE",
        "utbh3w3Nr8jG",
        "-jed0F0KsC0F",
        "in6RcmHhsRMk",
        "MFYjHZpCGigl",
        "O4Lz1Pbvl9iI",
        "VDJL0LMGIC6r",
        "j9xAymHkIRnv",
        "o18HZVcFGRF4",
        "KWsHfxWd6N2i",
        "-2JUmR5Z6N2j",
        "5L29fTV06N2o",
        "i-FDPUE16N2s",
        "P6ZkANTd6N2v",
        "Au7QRH9M6N26",
        "kXq4poIU6N29",
        "Sj4ioYr-6N3A",
        "NSk_edOO6N3E",
        "nviRMul26N3I",
        "oL5tsg6o6N3M",
        "cmzxRK9j6N3P",
        "Z_emxLrO6N3S"
      ],
      "mount_file_id": "11Xx7KVme-GfUbPRs3TJ4cqvsljMUy-i0",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
